{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4276037,"sourceType":"datasetVersion","datasetId":2507957},{"sourceId":10056829,"sourceType":"datasetVersion","datasetId":6197033},{"sourceId":13153631,"sourceType":"datasetVersion","datasetId":8333954},{"sourceId":588913,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":440358,"modelId":456908}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Super-Resolution and Image Restoration (img2img task) with SRRGAN \n\n*Ciao*! Welcome to this personal project, where I will implement a Generative Adversarial Network (GAN) to my previous [notebook](https://www.kaggle.com/code/marcorosato/super-resolution-and-restoration-of-images-u-net). I even modified the degradation function by adding more **degradation** and differentiating the degradations for **training** by making them random, while using fixed degradations for **validation**.\nI decided to use the **DF2K** dataset for training and validation, while **BSDS100** for the testing part.\n\nThe goal is to enhance a U-Net generator with an adversarial loss from a discriminator to produce sharper and more realistic images. \n\n**SRRGAN** stands for \"Super-Resolution and Restoration GAN\".","metadata":{"_uuid":"fc47449f-7163-4690-ba7b-c9ed17584f70","_cell_guid":"f885e2ab-8fdd-43a3-b116-2fbb3c274023","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Image Data Lifecycle\nTo fully understand the pipeline, which includes different functions and normalizations, it's helpful to trace the \"journey\" of an image. \n### 1. Training & Validation:\nTranining and validation are handled by the \"tf.data\" pipeline via the \"load_and_preprocess\" function.\n\n1. **File Path**: the process starts with a string representing the path to an image file (for ex. \"/kaggle/input/df2kdata/DF2K_train_HR/000001.png\")\n2. **Readn & Decode**:\n   *  `tf.io.read_file` reads the raw binary data of the image.\n   *  `tf.io.decode_png` decodes this data into a tf.Tensor of shape (H, W, 3) with pixel values in the range [0, 255].\n   *  `tf.image.convert_image_dtype(img, tf.float32)` converts the data type to float32 and normalizes the pixel values to the range [0, 1].\n3. **Patch Extraction and Augmentation**: a **256x256** High-Resolution (HR) patch is extracted from the full image (in training mode, augmentations are applied).\n4. **Degradation**: `degradation_pipeline_tf` function is called with the [0, 1] HR patch. All operations are performed directly on tensors:\n     * The pipeline stays entirely within the [0, 1] float32 range.\n     * Degradations (downsampling, blur, noise, JPEG) are applied using `tf.image` and `keras_cv` operations.\n     * The final Low-Resolution (LR) patch is clipped to ensure its values remain within the [0, 1] range.\n       \n   The output is a 128x128 LR patch as a float32 tensor in the [0, 1] range.\n5. **Final Normalization for Model Input**: both the LR and HR patches are scaled to match the model's expected input and output range (a `tanh` acivation function is used)\n    * LR Patch (Model Input `X`): scaled from [0, 1] to [-1, 1]\n    * HR Patch (Ground Truth `y_true`): Scaled from [0, 1] to [-1, 1]\n\n### 2. Inference & Visualization:\nThis process generates a super-resolved & restored image from the test set and displays the results.\n\n\n1. **Prepare Model Input**: load an HR image ([0, 1]) and run it through the degradation pipeline (`degrade_full_image`) to get an LR image ([0, 1]). This is then normalized to [-1, 1] to serve as the input for the generator model.\n2. **Model Prediction**: `model.predict()`  is called on the [-1, 1] normalized LR input. Thanks to the `tanh` at the end, the model's output (the predicted SR image) is in range [-1, 1].\n3. **Denormalization for Visualization**: `Matplotlib` can directly render float arrays with values in the [0, 1] range. To prepare images for plotting, the `denorm` function is used. It converts tensors from the [-1, 1] range to the [0, 1]\n4. **Display**: done by `plt.imshow()` to show a comparison of three images:\n    * LR input, converted from [-1, 1] to [0, 1] by the denorm function.\n    * SR predicted output, converted from [-1, 1] to [0, 1] by the denorm function.\n    * HR ground truth, which was already kept in the [0, 1] range.","metadata":{"_uuid":"49a3b16a-233f-433e-ad84-c10f5c22944a","_cell_guid":"3e5b0f4e-d36b-4761-819c-216d2754e3e9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- Install\n\n!pip install tqdm keras-cv protobuf==3.20.3 -q","metadata":{"_uuid":"9893e980-5316-4386-9cb7-b9dc27c5500e","_cell_guid":"803b43e9-15dd-4b8d-b804-fea5f2231642","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:20:26.927461Z","iopub.execute_input":"2025-09-25T18:20:26.927718Z","iopub.status.idle":"2025-09-25T18:20:30.274155Z","shell.execute_reply.started":"2025-09-25T18:20:26.927691Z","shell.execute_reply":"2025-09-25T18:20:30.272807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- LPIPS for Tensorflow\n# The standard PyTorch implementation is not used here because its dependencies conflict with the existing CUDA/CudNN setup.\n# If you have any suggestion how to implement it, please, let me know!\n\nimport sys\n\n!git clone https://github.com/Image-X-Institute/lpips_torch2tf.git\n\nsys.path.append(\"/kaggle/working/lpips_torch2tf\")","metadata":{"_uuid":"79a88b63-1332-43b7-b1ce-d73bca961b60","_cell_guid":"d79bb0e8-67da-45d6-9ce8-6a586e8994ef","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-25T18:20:30.275798Z","iopub.execute_input":"2025-09-25T18:20:30.27608Z","iopub.status.idle":"2025-09-25T18:20:30.534812Z","shell.execute_reply.started":"2025-09-25T18:20:30.27605Z","shell.execute_reply":"2025-09-25T18:20:30.533846Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Libraries\n\nimport os\nimport shutil\nimport glob\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\nimport math\nimport tensorflow as tf\nimport keras_cv\nfrom types import SimpleNamespace\nfrom matplotlib.gridspec import GridSpec\nfrom dev_src.loss_fns import lpips_base_tf\nfrom tqdm import tqdm\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras.saving import register_keras_serializable\nfrom tensorflow.keras.layers import SpectralNormalization\nfrom tensorflow.keras.callbacks import CSVLogger\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"45722e7d-5738-41c8-9d2b-98db2b213c0d","_cell_guid":"131237ac-caf9-4fc7-8de7-d7451d87d733","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-25T18:20:30.537205Z","iopub.execute_input":"2025-09-25T18:20:30.537473Z","iopub.status.idle":"2025-09-25T18:20:30.54381Z","shell.execute_reply.started":"2025-09-25T18:20:30.53745Z","shell.execute_reply":"2025-09-25T18:20:30.543052Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### --- 1 Configuration\n\n# 1.1 Seeds\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)\n\n# 1.2 Set the global policy for mixed precision\npolicy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\ntf.keras.mixed_precision.set_global_policy(policy)\n\n# 1.3 Patch dimensions for High-Resolution (HR) and Low-Resolution (LR) images\nPATCH_H = 256                          # Height of the HR image patches\nPATCH_W = 256                          # Width of the HR image patches\nPATCH_LR_H = PATCH_H // 2              # Height of the LR image patches (128)\nPATCH_LR_W = PATCH_W // 2              # Width of the LR image patches (128)\nUPSCALE_FACTOR = 2                     # scale factor (2x super-resolution)\nCH = 3                                 # RGB\n\n# 1.4 Training\nBATCH_SIZE      = 64\nEPOCHS          = 100\nSAMPLE_SIZE     = None                 # None = full data\n\n# 1.5 Loss function weights \nPERCEPTUAL_WEIGHT_PRE = 1e-3\n\nPIXEL_WEIGHT_GAN = 20.0\nPERCEPTUAL_WEIGHT_GAN = 5e-2\nGLOBAL_GAN_WEIGHT = 5e-2","metadata":{"_uuid":"be436540-4cdb-4a58-ae5b-d0e457b8e366","_cell_guid":"fba3347f-7a9e-426f-8d5c-703c20c8bca1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:20:30.544657Z","iopub.execute_input":"2025-09-25T18:20:30.544906Z","iopub.status.idle":"2025-09-25T18:20:30.628112Z","shell.execute_reply.started":"2025-09-25T18:20:30.544889Z","shell.execute_reply":"2025-09-25T18:20:30.627468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading and Preprocessing\nThis section handles the steps of preparing the dataset for the model task. The pipeline is designed by creating paired low-resolution (LR) and high-resolution (HR) image patches for training.\n\nThe key stages are:\n1.  **Path Collection and Splitting:** Gather the file paths of all images and split them into training, validation, and test sets.\n2.  **Dataset Sanity Check:** Verify the minimum dimensions of the images to ensure the patch extraction is valid.\n3.  **Degradation Function:** A custom function is defined to simulate real-world image degradation and creating realistic LR images from HR patches. This includes blur, downsampling, noise and JPEG compression.\n4.  **\"tf.data\" Pipeline:** An input pipeline using \"tf.data.Dataset\" that handles image loading, augmentation, degradation, batching, and prefetching.","metadata":{"_uuid":"7bf640f9-89ae-4fb6-bf3c-e15b0bd35307","_cell_guid":"ec955957-9128-4a87-ae1f-bd021d895ce0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Image Path Collection and Dataset Splitting\nScan the dataset directory to collect the file paths of all available PNG images. \n-   **Training Set:** DF2K_train_HR.\n-   **Validation Set:** DF2K_valid_HR.\n-   **Test Set:** BSDS100.","metadata":{"_uuid":"d108c355-2024-44bb-bc40-c8c556f4aa0f","_cell_guid":"559b049a-8331-4061-b12f-e78ba779a34c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 2 Collection of image paths\n\ntrain_folder = \"/kaggle/input/df2kdata/DF2K_train_HR\"\nvali_folder = \"/kaggle/input/df2kdata/DF2K_valid_HR\"\ntest_folder = \"/kaggle/input/super-resolution-benchmarks/BSDS100/BSDS100\"\n\ntrain_paths = []\nval_paths = []\ntest_paths =[]\n\ntrain = os.path.join(train_folder, f\"*.png\")            # All the images are png format\nfor f in glob.glob(train):\n        if os.path.getsize(f) > 0:                      # Ensure that each file is not empty (>0)\n           train_paths.append(f)\n\nvalidation = os.path.join(vali_folder, f\"*.png\")        \nfor f in glob.glob(validation):\n        if os.path.getsize(f) > 0:                      \n           val_paths.append(f)\n\ntest = os.path.join(test_folder, f\"*.png\")              \nfor f in glob.glob(test):\n        if os.path.getsize(f) > 0:                      \n           test_paths.append(f)\n\nprint(f\"Training images from DF2K train: {len(train_paths)}\")\nprint(f\"Validation images from DF2K val: {len(val_paths)}\")\nprint(f\"Test images from BSDS100: {len(test_paths)}\")","metadata":{"_uuid":"b2525169-24ef-453d-9d64-996927b5e618","_cell_guid":"c83569fe-bdae-4256-aab2-e617fe2482f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:20:30.629632Z","iopub.execute_input":"2025-09-25T18:20:30.629872Z","iopub.status.idle":"2025-09-25T18:20:32.211812Z","shell.execute_reply.started":"2025-09-25T18:20:30.629857Z","shell.execute_reply":"2025-09-25T18:20:32.211145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Sanity Check\nPerform a sanity check on the image dimensions across the entire DF2K dataset. This step iterates through all images to find the minimum height and width. This is done to confirm that my chosen HR patch size (256x256) can be safely extracted from every image.","metadata":{"_uuid":"65641b0e-6db8-4ed3-bcfa-c55a08720e09","_cell_guid":"d3db6caa-cc08-424e-8a36-d10fd24cde0a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 3 Minimum height and width between all images.\n# This step helps to understand the variability in image sizes and ensure that the chosen patch size (256x256) is feasible\n# by checking if all images are at least as large as the desired patch size.\n\nmin_height = np.inf\nmin_width  = np.inf\n\nfor path in train_paths + val_paths:\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)      # Read image by keeping its original channel count\n    if img is None:\n        print(f\"Can't read this image: {path}\")\n        continue\n\n    h, w = img.shape[:2]                              #Get height and width\n    if h < min_height:\n        min_height = h\n    if w < min_width:\n        min_width = w\n\nif min_height == np.inf or min_width == np.inf:\n    print(\"No valid image\")\nelse:\n    print(f\"Minimum hight between all images : {min_height}\")\n    print(f\"Minimum width between all images : {min_width}\")","metadata":{"_uuid":"8bf6e8f1-3ded-4a48-8cfd-f52e328f26d2","_cell_guid":"d4bfcec7-ce28-4d53-a8f1-9720a2894611","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:20:32.212543Z","iopub.execute_input":"2025-09-25T18:20:32.212811Z","iopub.status.idle":"2025-09-25T18:25:42.112532Z","shell.execute_reply.started":"2025-09-25T18:20:32.212785Z","shell.execute_reply":"2025-09-25T18:25:42.111761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Degradation Function - Blind Restoration Task\nTo train the super-resolution model, we require pairs of low-resolution (LR) and high-resolution (HR) images. This is accomplished by the degradation_pipeline_tf, a function that synthesizes a LR image from an HR source tensor (in the range [0, 1]).\n\nA key design principle of this pipeline is its dual-mode operation (controlled by an is_training flag):\n\n\n* Training Mode (Stochastic): When training, the pipeline applies a sequence of degradations with randomized parameters and even randomized order. This serves to improve generalization capabilities.\n\n* Validation/Testing Mode (Deterministic): During evaluation, the pipeline uses a fixed set of parameters and a deterministic order. This ensures that the model's performance is measured consistently and reproducibly across epochs.\n\n\nThe degradation sequence is as follows:\n\n\n1. **Blur (First-Order Degradation):** This step simulates physical lens imperfections.\n    * *Training:* A blur type is chosen randomly (50% chance each):\n        * Gaussian Blur: Uses a random kernel size (from 3x3 to 7x7) and a random sigma factor (from 0.2 to 1.2).\n        * Box Blur: Implemented via a depthwise convolution with a random odd-sized kernel (from 3x3 to 5x5).\n    * *Validation:* A single, fixed Gaussian blur is applied (5x5 kernel, sigma 1.0).\n2. **Downsampling:** The blurred HR image is downscaled (x2)\n    * *Training:* The interpolation method is chosen randomly from Bilinear, Bicubic, and Area to simulate different downsampling algorithms.\n    * *Validation:* A fixed Area interpolation is used, which is generally preferred for downscaling as it prevents aliasing.\n3. **Post-Downsampling Degradations (Second-Order):** After downsampling, a sequence of noise and compression artifacts is applied. The order of these operations is also randomized during training.\n    * *Training:*\n       * Order: Randomly applies either (Noise → JPEG) or (JPEG → Noise).\n       * Noise: Randomly adds one of two types:\n            * Gaussian Noise: With a random standard deviation (from 1/255 to 15/255).\n            * Poisson Noise: Simulates shot noise with a random scale factor (from 10.0 to 60.0).\n       * JPEG Compression: Applied with an 80% probability. If applied, the quality is a random integer between 60 and 95.\n    * *Validation:*\n        * Order: A fixed order is used: Noise → JPEG.\n        * Noise: Fixed Gaussian noise is added (standard deviation of 10/255).\n        * JPEG Compression: Fixed JPEG compression is applied with a quality of 75.\n4. **Clipping:** The final LR image's pixel values are clipped to the [0, 1] range to ensure a valid output.","metadata":{"_uuid":"05da835b-d056-4ed4-b30e-ecfba6136090","_cell_guid":"fe1183ef-243b-4a46-94cc-1bdcf77702cc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 4 Degradation Function for TRAINING/VALIDATION\n# This function creates a low-resolution image from a high-resolution one.\n# It takes an HR image tensor in the [0, 1] range and returns a degraded LR version, also in [0, 1].\n\n# Random blur inizialization\nrandom_blur_layer = keras_cv.layers.RandomGaussianBlur(\n    kernel_size = (3, 7),   # KerasCV layer picks a random kernel size...\n    factor = (0.2, 1.2)     # ... And sigma for variety\n)\n\n# Fixed blur inizialization\nfixed_blur_layer = keras_cv.layers.RandomGaussianBlur(\n    kernel_size=(5, 5), # A fixed kernel size\n    factor=1.0          # A fixed sigma factor\n)\n\n# Box blur\nbox_blur_kernel_size = (3, 5)\n\n\n@tf.function\ndef degradation_pipeline_tf(hr_patch, is_training):\n    \"\"\"\n    Applies a degradation pipeline that is random for training and\n    deterministic for validation.\n    \"\"\"\n    # Ensure is_training is a TensorFlow boolean tensor for tf.cond\n    is_training = tf.convert_to_tensor(is_training, dtype=tf.bool)\n\n\n    # --- 4.1 Blur \n    # 4.1.1 Gaussian blur (lens softness)\n    def apply_random_gaussian_blur(img):\n        # The KerasCV layer needs a batch dimension. (add here and then removed)\n        original_dtype = img.dtype\n        img_batched = tf.expand_dims(img, axis=0)\n        blurred_img = random_blur_layer(img_batched, training=True)\n        return tf.cast(blurred_img[0], dtype=original_dtype)\n    \n    # 4.1.2 Box blur\n    def apply_random_box_blur(image):\n    \n        # 1. Generate the random kernel size as a TensorFlow tensor\n        kernel_size = tf.random.uniform((), \n                                    minval=box_blur_kernel_size[0], \n                                    maxval=box_blur_kernel_size[1], \n                                    dtype=tf.int32)\n        # Ensure the kernel size is odd\n        kernel_size = kernel_size + (1 - kernel_size % 2) \n    \n        # 2. Get the number of channels from the image shape\n        channels = tf.shape(image)[-1]\n    \n        # 3. Create the box blur kernel dynamically\n        # The value for each pixel in the kernel should be 1 / (size*size) to average\n        kernel_value = 1.0 / tf.cast(kernel_size * kernel_size, image.dtype)\n        # Create the kernel tensor for depthwise convolution\n        # Shape is [height, width, in_channels, channel_multiplier=1]\n        kernel = tf.fill([kernel_size, kernel_size, channels, 1], kernel_value)\n    \n        # Add a batch dimension to the input image for the convolution op\n        image_batched = tf.expand_dims(image, axis=0)\n    \n        # 4. Apply the convolution\n        blurred_image = tf.nn.depthwise_conv2d(\n            image_batched,\n            kernel,\n            strides=[1, 1, 1, 1],\n            padding=\"SAME\"\n        )\n    \n        # Remove the batch dimension before returning\n        return tf.squeeze(blurred_image, axis=0)\n    \n    \n    # 4.1.3 Fixed Gaussian blur\n    def apply_fixed_blur(img):\n        original_dtype = img.dtype\n        img_batched = tf.expand_dims(img, axis=0)\n        blurred_img = fixed_blur_layer(img_batched, training=False) # Ensure it's deterministic\n        return tf.cast(blurred_img[0], dtype=original_dtype)\n    \n    # 4.1.4 Choice of what blur to apply\n    def apply_randomized_blur(img):\n        blur_type = tf.random.uniform(())\n        return tf.cond(blur_type < 0.5,     # 50% gaussian blur and 50% box blur\n                       lambda: apply_random_gaussian_blur(img),\n                       lambda: apply_random_box_blur(img))\n    \n    # 4.1.5 Apply the blur to the HR patch\n    hr_patch = tf.cond(is_training,\n                       lambda: apply_randomized_blur(hr_patch),  # If training is True\n                       lambda: apply_fixed_blur(hr_patch))       # If training is False\n    \n    \n    \n    \n    \n    # --- 4.2 Downsample the image by the upscale factor using area interpolation (Randomized)\n    lr_h = PATCH_H // UPSCALE_FACTOR\n    lr_w = PATCH_W // UPSCALE_FACTOR\n    \n    def random_resize(img):\n        rand_interpo = tf.random.uniform((), 0, 3, dtype=tf.int32)\n        # Use the pre-calculated static dimensions\n        return tf.switch_case(rand_interpo, {\n            0: lambda: tf.image.resize(img, [lr_h, lr_w], method=tf.image.ResizeMethod.BILINEAR),\n            1: lambda: tf.image.resize(img, [lr_h, lr_w], method=tf.image.ResizeMethod.BICUBIC),\n            2: lambda: tf.image.resize(img, [lr_h, lr_w], method=tf.image.ResizeMethod.AREA),\n        })\n    \n    def fixed_resize(img):\n        # Use a consistent for validation, with the pre-calculated static dimensions\n        return tf.image.resize(img, [lr_h, lr_w], method=tf.image.ResizeMethod.AREA)\n    \n    lr_patch = tf.cond(is_training,\n                       lambda: random_resize(hr_patch),\n                       lambda: fixed_resize(hr_patch))\n    \n    \n    \n    # --- 4.3 Random order Noise (Gaussian and Poisson) and JPEG\n    # --- 4.3.1 Noise for training\n    def apply_random_noise(img):\n        # 4.3.1.1 Gaussian\n        def add_gaussian_noise():\n            noise_std = tf.random.uniform([], 1/255.0, 15/255.0)\n            noise = tf.random.normal(tf.shape(img), 0.0, noise_std, dtype=img.dtype)\n            return img + noise\n        # 4.3.1.2 Poisson\n        def add_poisson_noise():\n            noise_scale = tf.random.uniform([], 10.0, 60.0)\n            noisy_image = tf.random.poisson(shape=[], lam=img * noise_scale) / noise_scale\n            return tf.cast(noisy_image, img.dtype)\n    \n        return tf.cond(tf.random.uniform(()) < 0.5,\n                       true_fn=add_gaussian_noise,\n                       false_fn=add_poisson_noise)\n    # 4.3.2 Noise for val\n    def apply_fixed_noise(img):\n        noise_std = tf.constant(10/255.0)\n        noise = tf.random.normal(tf.shape(img), 0.0, noise_std, dtype=img.dtype)\n        return img + noise\n    \n    # --- 4.3.3 JPEG \n    # 4.3.3.1 JPEG for training\n    def apply_random_jpeg(img):\n        img_float32 = tf.cast(img, tf.float32)\n        img_uint8 = tf.image.convert_image_dtype(img_float32, tf.uint8, saturate=True)\n        def apply_jpeg_with_known_shape():\n            jpeg_img = tf.image.random_jpeg_quality(img_uint8, min_jpeg_quality=60, max_jpeg_quality=95)\n            return tf.ensure_shape(jpeg_img, img_uint8.shape)\n    \n        img_jpeg = tf.cond(tf.random.uniform([], 0.0, 1.0) < 0.8,\n                           true_fn=apply_jpeg_with_known_shape,\n                           false_fn=lambda: img_uint8)\n        return tf.image.convert_image_dtype(img_jpeg, img.dtype)\n    \n    # 4.3.3.2 JPEG for val\n    def apply_fixed_jpeg(img):\n        jpeg_quality = 75\n        image_float32 = tf.cast(img, tf.float32)\n        img_uint8 = tf.image.convert_image_dtype(image_float32, tf.uint8, saturate=True)\n        img_jpeg = tf.image.adjust_jpeg_quality(img_uint8, jpeg_quality=jpeg_quality)\n        return tf.image.convert_image_dtype(img_jpeg, img.dtype)\n        \n    # --- 4.4 Logic for random order\n    def apply_post_degradations_randomized(img):\n        # 4.4.1 Noise and then Jpeg\n        def order_noise_then_jpeg(x):\n            x = apply_random_noise(x)\n            x = apply_random_jpeg(x)\n            return x\n        # 4.4.2 Jpeg and then Noise\n        def order_jpeg_then_noise(x):\n            x = apply_random_jpeg(x)\n            x = apply_random_noise(x)\n            return x\n        \n        # 4.4.3 50% one of these order\n        return tf.cond(tf.random.uniform(()) < 0.5,\n                       lambda: order_noise_then_jpeg(img),\n                       lambda: order_jpeg_then_noise(img))\n    \n    def apply_post_degradations_fixed(img):\n        # Fixed for valiidation\n        img = apply_fixed_noise(img)\n        img = apply_fixed_jpeg(img)\n        return img\n    \n    # 4.4.4 Apply post downsample degradation\n    lr_patch = tf.cond(is_training,\n                       lambda: apply_post_degradations_randomized(lr_patch),\n                       lambda: apply_post_degradations_fixed(lr_patch))\n    \n    \n    \n    \n    \n    # 4.5 Clip the final result to ensure all pixel values are in the valid [0, 1] range\n    lr_patch_final = tf.clip_by_value(lr_patch, 0.0, 1.0)\n    \n    return lr_patch_final","metadata":{"_uuid":"7d6eaf15-9760-489c-8d1b-416b6e1abb3c","_cell_guid":"11f67500-f259-4e8e-86b0-d8c42a5d141f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:42.113482Z","iopub.execute_input":"2025-09-25T18:25:42.113715Z","iopub.status.idle":"2025-09-25T18:25:42.136382Z","shell.execute_reply.started":"2025-09-25T18:25:42.113698Z","shell.execute_reply":"2025-09-25T18:25:42.135764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4.6 Degradation Function for TEST: Inference & Final Visualization\n\n@tf.function\ndef degrade_full_image(hr_image):\n    \"\"\"\n    Applies a deterministic degradation pipeline to a full-sized HR image,\n    preserving its aspect ratio.\n    \"\"\"\n    # 1. Calculate target LR dimensions based on the input HR image shape\n    hr_shape = tf.shape(hr_image)\n    lr_h = hr_shape[0] // UPSCALE_FACTOR\n    lr_w = hr_shape[1] // UPSCALE_FACTOR\n\n    # 2. Apply the same FIXED blur as in the validation pipeline\n    def apply_fixed_blur(img):\n        original_dtype = img.dtype\n        img_batched = tf.expand_dims(img, axis=0)\n        blurred_img = fixed_blur_layer(img_batched, training=False)\n        return tf.cast(blurred_img[0], dtype=original_dtype)\n    \n    hr_blurred = apply_fixed_blur(hr_image)\n\n    # 3. Downsample using the CALCULATED dimensions to preserve aspect ratio\n    lr_patch = tf.image.resize(hr_blurred, [lr_h, lr_w], method=tf.image.ResizeMethod.AREA)\n\n    # 4. Apply the same FIXED post-degradations (noise -> JPEG)\n    def apply_fixed_noise(img):\n        noise_std = tf.constant(10/255.0)\n        noise = tf.random.normal(tf.shape(img), 0.0, noise_std, dtype=img.dtype)\n        return img + noise\n    \n    def apply_fixed_jpeg(img):\n        jpeg_quality = 75\n        image_float32 = tf.cast(img, tf.float32)\n        img_uint8 = tf.image.convert_image_dtype(image_float32, tf.uint8, saturate=True)\n        img_jpeg = tf.image.adjust_jpeg_quality(img_uint8, jpeg_quality=jpeg_quality)\n        return tf.image.convert_image_dtype(img_jpeg, img.dtype)\n\n    lr_patch = apply_fixed_noise(lr_patch)\n    lr_patch = apply_fixed_jpeg(lr_patch)\n    \n    # 5. Clip the final result\n    lr_patch_final = tf.clip_by_value(lr_patch, 0.0, 1.0)\n    \n    return lr_patch_final","metadata":{"_uuid":"2fce1e69-147d-4fe0-adc6-57eabebc6fca","_cell_guid":"00fcd0dd-db1e-4e9b-af29-d09391c8bdd5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-25T18:25:42.137327Z","iopub.execute_input":"2025-09-25T18:25:42.137689Z","iopub.status.idle":"2025-09-25T18:25:42.157113Z","shell.execute_reply.started":"2025-09-25T18:25:42.137664Z","shell.execute_reply":"2025-09-25T18:25:42.156578Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5 Visualization of the degradation function\n\n%matplotlib inline\n\n# 5.1 Select a random image from train\nrandom_path = random.choice(train_paths)\nhr_full = tf.image.convert_image_dtype(tf.io.decode_png(tf.io.read_file(random_path), channels=CH), tf.float32)\nhr_patch = tf.image.resize_with_crop_or_pad(hr_full, PATCH_H, PATCH_W)\n\n# 5.2 Apply  degradation\nlr_patch_degraded = degradation_pipeline_tf(hr_patch, is_training=tf.constant(True))\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 3, 1)\nplt.title(\"HR full [0, 1]\")\nplt.imshow(hr_full.numpy().astype('float32'))\nplt.axis(\"off\")\nplt.subplot(1, 3, 2)\nplt.title(\"HR Patch [0, 1]\")\nplt.imshow(hr_patch.numpy().astype('float32'))\nplt.axis(\"off\")\nplt.subplot(1, 3, 3)\nplt.title(\"LR Patch [0, 1] (degraded)\")\nplt.imshow(lr_patch_degraded.numpy().astype('float32'))\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"45294371-f0dd-426f-9e7c-c57d4d4065ae","_cell_guid":"e955c8aa-ed74-4b5b-99fa-d4fdcfbe5ad1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:42.160014Z","iopub.execute_input":"2025-09-25T18:25:42.160576Z","iopub.status.idle":"2025-09-25T18:25:44.763373Z","shell.execute_reply.started":"2025-09-25T18:25:42.160551Z","shell.execute_reply":"2025-09-25T18:25:44.76264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## \"tf.data\" Preprocessing Function\n\nThe `load_and_preprocess_train` and `load_and_preprocess_eval` functions form the core of the data pipeline. The first function processes the training data with augmentations, while the second handles validation/test data with deterministic cropping.\n\n 1.  **Load and Normalize to [0, 1]:** Reads an image file and converts it to float32 in the [0, 1] range.\n 2.  **Patch Extraction and Augmentation (ONLY TRAINING):** Extracts an HR patch ([0, 1]).\n 3.  **Degradation:** The HR patch is passed to our degradation_pipeline_tf function to generate the corresponding LR patch ([0, 1]).\n 4.  **Final Normalization to [-1, 1]:** Both the LR and HR patches are normalized to the [-1, 1] range, which is the format the model expects for training.","metadata":{"_uuid":"b5bdbc41-4eb7-41e1-b6df-35e117d74002","_cell_guid":"763b55a3-cbab-487d-94d6-4f554457691c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 6 Data loading and preprocessing functions\n\n# --- This function is ONLY for the training dataset\n@tf.function\ndef load_and_preprocess_train(path):\n    # 6.1 Read and decode HR image to [0, 1]\n    img = tf.io.read_file(path)\n    img = tf.io.decode_png(img, channels=CH)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n\n    # --- DATA AUGMENTATION ---\n    \n    # 6.2 Extract HR 256×256 patch and apply augmentations (still [0, 1])\n    # Crop\n    hr_patch = tf.image.random_crop(img, [PATCH_H, PATCH_W, CH])\n    # Horizontal Flip         \n    hr_patch = tf.image.random_flip_left_right(hr_patch)\n    # Vertical Flip\n    hr_patch = tf.image.random_flip_up_down(hr_patch)\n    # Rotation (0, 90, 180, 270 degrees)\n    k = tf.random.uniform([], 0, 4, tf.int32)\n    hr_patch = tf.image.rot90(hr_patch, k)\n    # --- Colour Augmentations \n    hr_patch = tf.image.random_brightness(hr_patch, max_delta=0.05)\n    hr_patch = tf.image.random_contrast(hr_patch, lower=0.9, upper=1.1)\n    hr_patch = tf.image.random_saturation(hr_patch, lower=0.9, upper=1.1)\n    hr_patch = tf.image.random_hue(hr_patch, max_delta=0.06)\n    hr_patch = tf.clip_by_value(hr_patch, 0.0, 1.0)\n    \n    # --- END DATA AUGMENTATION ---\n    \n    \n    # 6.3 Generate LR patch [0, 1] using the degradation pipeline (with is_training=True)\n    lr_patch_0_1 = degradation_pipeline_tf(hr_patch, is_training=tf.constant(True))\n\n    # 6.4 Normalize both patches to [-1, 1] for the model\n    lr_patch = (lr_patch_0_1 * 2.0) - 1.0\n    hr_patch_norm = (hr_patch * 2.0) - 1.0\n\n    # 6.5 Set static shapes\n    lr_patch.set_shape([PATCH_LR_H, PATCH_LR_W, CH])\n    hr_patch_norm.set_shape([PATCH_H, PATCH_W, CH])\n\n    return lr_patch, hr_patch_norm\n\n\n\n\n# --- This function is ONLY for the validation/test dataset\n@tf.function\ndef load_and_preprocess_eval(path):\n    # 6.1 Read and decode HR image to [0, 1]\n    img = tf.io.read_file(path)\n    img = tf.io.decode_png(img, channels=CH)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    # 6.2 For validation/testing, just do a center crop\n    hr_patch = tf.image.resize_with_crop_or_pad(img, PATCH_H, PATCH_W)\n\n    # 6.3 Generate LR patch [0, 1] using the degradation pipeline (with is_training=False)\n    lr_patch_0_1 = degradation_pipeline_tf(hr_patch, is_training=tf.constant(False))\n\n    # 6.4 Normalize both patches to [-1, 1] for the model\n    lr_patch = (lr_patch_0_1 * 2.0) - 1.0\n    hr_patch_norm = (hr_patch * 2.0) - 1.0\n\n    # 6.5 Set static shapes\n    lr_patch.set_shape([PATCH_LR_H, PATCH_LR_W, CH])\n    hr_patch_norm.set_shape([PATCH_H, PATCH_W, CH])\n\n    return lr_patch, hr_patch_norm","metadata":{"_uuid":"aba3d0de-e5c0-428b-9d82-f999052ef889","_cell_guid":"7be36a0c-f295-46e4-af09-0901c714b31b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:44.764254Z","iopub.execute_input":"2025-09-25T18:25:44.76453Z","iopub.status.idle":"2025-09-25T18:25:44.777504Z","shell.execute_reply.started":"2025-09-25T18:25:44.76451Z","shell.execute_reply":"2025-09-25T18:25:44.776778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building the Datasets\nUsing the preprocessing function defined above, I now construct the final datasets for training, validation, and testing.","metadata":{"_uuid":"79b17065-3bab-4ffb-a3b2-ef60639e211d","_cell_guid":"df71e32a-7dcb-4311-a3bb-840a93b192ad","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 7 Dataset creation\n\ndef make_dataset(paths, training):\n    ds = tf.data.Dataset.from_tensor_slices(paths)\n    if training:\n        ds = ds.shuffle(buffer_size=len(paths))\n        # Use the training preprocessor\n        map_func = load_and_preprocess_train\n    else:\n        # Use the evaluation preprocessor\n        map_func = load_and_preprocess_eval\n        \n    ds = ds.map(map_func, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n    return ds\n\nif SAMPLE_SIZE is not None:\n    train_paths = train_paths[:SAMPLE_SIZE]\n    val_paths   = val_paths[:SAMPLE_SIZE//5]\n    test_paths  = test_paths[:SAMPLE_SIZE//5]\n\n# Creation\ntrain_ds = make_dataset(train_paths, training=True)\nval_ds   = make_dataset(val_paths,   training=False)\ntest_ds  = make_dataset(test_paths,  training=False)\n\nprint(f\"Train: {len(train_paths)}, Val: {len(val_paths)}, Test: {len(test_paths)}\")\nprint(train_ds)\nprint(val_ds)\nprint(test_ds)","metadata":{"_uuid":"fb054245-3541-434d-9e60-1cded45df0fc","_cell_guid":"75276417-bb78-4c43-957f-5b2d7ef18d49","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:44.778554Z","iopub.execute_input":"2025-09-25T18:25:44.77906Z","iopub.status.idle":"2025-09-25T18:25:45.359059Z","shell.execute_reply.started":"2025-09-25T18:25:44.77904Z","shell.execute_reply":"2025-09-25T18:25:45.358447Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architectures\n\n### Generator (U-Net)\nThe U-Net is a powerful convolutional neural network characterized by its U-shaped encoder-decoder architecture. Its unique feature are the skip connections: they connect encoder layers with corresponding decoder layers, allowing the recovery of spatial information lost during the downsampling phase.\n\nMy model utilizes a \"hybrid\" U-Net architecture. The standard encoder-decoder structure with skip connections is used for its ability in image restoration tasks, allowing for the fusion of features from multiple spatial scales. To enhance its super-resolution capabilities, I replaced the traditional simple bottleneck with a deep stack of residual blocks. This design is inspired by state-of-the-art super-resolution networks and acts as a feature extractor, focusing on learning the residual high-frequency information.\n\nKey features include:\n* **Squeeze-and-Excitation (channel attention):** this block learns to give more \"weight\" to the most important channels (features);\n* **Spatial Attention (spatial attention):** highlights spatial locations (helps the model focus on edges and textures);\n* **PixelShuffle:** An enhanced upsampling method compared to Conv2DTranspose;\n* **Global residaul:** A final residual connection where a simple upsampled version of the input is added to the network's output.\n\n\n### Discriminator (PatchGAN)\nThe discriminator's job is to distinguish between real HR images and the fake SR images produced by the generator. Instead of classifying the entire image with a single \"real\" or \"fake\" label, a **PatchGAN** discriminator classifies N x N patches of the input image. This encourages the generator to produce realistic details across the entire image, rather than just getting the global structure right. The output is a feature map where each \"pixel\" corresponds to a verdict on a patch of the original image.","metadata":{"_uuid":"a6d0bc68-ada9-4f68-ab82-863e2a4a2e21","_cell_guid":"752f2050-9896-446b-9346-dd8baaaebd3d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 8.1 Generator (U-Net) Architecture\n\nNUM_HEAD_BLOCKS = 4\nNUM_BOTTLENECK_BLOCKS = 5\nNUM_TAIL_BLOCKS = 2\n\n# Squeeze-and-Excitation. \n# Squeeze-and-Excitation Networks\" (SENet) by \"Jie Hu et al., 2018 (CVPR)\" \ndef se_block(input_tensor, ratio=8):\n    channels = input_tensor.shape[-1]\n    se = layers.GlobalAveragePooling2D()(input_tensor)                                   # Squeeze: Aggregates spatial information into a single value per channel\n    se = layers.Reshape((1, 1, channels))(se)                                            # Vector of shape (1,1,C)\n    se = layers.Conv2D(channels // ratio, 1, activation = \"relu\", use_bias=True)(se)     # Excitation: Learn a nonlinear gating function to weight the channels.\n    se = layers.Conv2D(channels, 1, activation = \"sigmoid\", use_bias=True)(se)           # 1x1 convolution that restores the number of channels back to the original C\n    return layers.Multiply()([input_tensor, se])                                         # Apply weights to the input tensor\n\n\n# Spatial Attention Block\n# CBAM: Convolutional Block Attention Module by \"Woo, S., Park, J., Lee, JY., Kweon, I.S. (2018)\"\n@register_keras_serializable()\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.concat = layers.Concatenate(axis=3)\n        self.multiply = layers.Multiply()\n\n    def build(self, input_shape):\n        self.conv = layers.Conv2D(\n            filters=1, \n            kernel_size=7, \n            padding='same', \n            activation='sigmoid'\n        )\n        super().build(input_shape)\n\n    def call(self, inputs):\n        avg_pool = tf.reduce_mean(inputs, axis=3, keepdims=True)       # Average pool\n        max_pool = tf.reduce_max(inputs, axis=3, keepdims=True)        # Max pool\n        concat = self.concat([avg_pool, max_pool])                     # Concatenate\n        attention_map = self.conv(concat)                              # Convolution\n        return self.multiply([inputs, attention_map])\n\n    def get_config(self):                                              # Save\n        base_config = super().get_config()\n        return base_config\n\n        \n# PixelShuffle\n# Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by \"Wenzhe Shi et al., 2016 (CVPR)\"\n@register_keras_serializable()\nclass PixelShuffle(layers.Layer): \n    def __init__(self, block_size, **kwargs):                       # Store the configuration of the layer (runs only once)\n        super().__init__(**kwargs)\n        self.block_size = block_size                                # It takes the block_size (upscale factor) and saves it as self.block_size so it can be used later\n\n    def call(self, inputs):                                         # Runs every time you pass a tensor through the layer\n        return tf.nn.depth_to_space(inputs, self.block_size)        # This is the upsample part (uses the self.block_size that was saved during __init__)\n\n    def get_config(self):                                           # Saving the Model (model.save())\n        config = super().get_config()\n        config.update({\"block_size\": self.block_size})              # It returns a dictionary of the layer's configuration\n        return config\n\n\n# Attention block in Head & Tail\ndef head_tail_block(x, filters=64):\n    identity = x\n    \n    # Process \n    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)(x)\n    x = layers.GroupNormalization(groups=32)(x)\n    x = layers.LeakyReLU(0.1)(x)\n    \n    # Attention\n    x = se_block(x)\n    x = SpatialAttentionBlock()(x)\n    \n    # Residual connection\n    return layers.Add()([identity, x])\n\n\n\n\n\n# Encoder \ndef enc_block(x, filters):\n    x = layers.Conv2D(filters, 3, padding = \"same\", use_bias=False)(x)\n    x = layers.GroupNormalization(groups=32)(x)\n    x = layers.LeakyReLU(0.1)(x)\n    x = layers.Conv2D(filters, 3, strides = 2, padding = \"same\", use_bias=False)(x)      # \"stride=2\" -> downsample\n    x = layers.GroupNormalization(groups=32)(x)\n    x = layers.LeakyReLU(0.1)(x)\n    x = se_block(x)\n    return x\n\n# Bottleneck\ndef bot_block(x, filters):\n    identity = x\n    b = layers.Conv2D(filters, 3, padding = \"same\", use_bias=False)(x)\n    b = layers.GroupNormalization(groups=32)(b)\n    b = layers.LeakyReLU(0.1)(b)\n    b = layers.SpatialDropout2D(0.05)(b)                    # Regularization\n    residual_bottleneck = layers.Conv2D(filters, 3, padding = \"same\", use_bias=False)(b)\n    residual_bottleneck = layers.GroupNormalization(groups=32)(residual_bottleneck)\n    b = layers.Add()([x, residual_bottleneck])              # Residual connection\n    b = layers.LeakyReLU(0.1)(b)\n    b_channel_att = se_block(b)\n    b_spatial_att = SpatialAttentionBlock()(b_channel_att) \n    return b_spatial_att\n     \n# Decoder\ndef dec_block(x, skip, filters):\n    x = layers.Conv2D(filters * (UPSCALE_FACTOR**2), 3, padding=\"same\", use_bias=True)(x)   # Before upsample increase the number of channels\n    x = PixelShuffle(block_size=UPSCALE_FACTOR)(x)           # Upscale by Pixelshuffle\n    x = layers.LeakyReLU(0.1)(x)\n    x = layers.Concatenate()([x, skip])                      # Skip connection (core of the U-Net)\n    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)(x)\n    x = layers.GroupNormalization(groups=32)(x)    \n    x = layers.LeakyReLU(0.1)(x)\n    x = layers.SpatialDropout2D(0.05)(x)                     # Regularization\n    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)(x)\n    x = layers.GroupNormalization(groups=32)(x)\n    x = layers.LeakyReLU(0.1)(x)\n    return x\n\n\n\n\n\ndef build_unet_sr_generator():\n    inputs = layers.Input((None, None, CH))  \n\n    # Global Residual Learning (START).\n    # \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\" by \"Christian Ledig et al., 2017 (CVPR)\"\n    upsampled_lr_base = layers.UpSampling2D(size = (UPSCALE_FACTOR, UPSCALE_FACTOR), interpolation = \"bilinear\")(inputs)\n\n    s0_initial = layers.Conv2D(64, 3, padding =\"same\", use_bias=True)(inputs)\n    s0_initial = layers.LeakyReLU(0.1)(s0_initial)\n\n    # Head block\n    head_features = s0_initial\n    for _ in range(NUM_HEAD_BLOCKS):\n        head_features = head_tail_block(head_features, filters=64)\n    head_output = layers.Conv2D(64, 3, padding=\"same\")(head_features)\n    \n    s0_sum = layers.Add()([head_output, s0_initial])\n    s0_final = layers.LeakyReLU(0.1)(s0_sum)\n\n    # Encoder\n    e1 = enc_block(s0_final, 64)                        \n    e2 = enc_block(e1, 128)                       \n    e3 = enc_block(e2, 256)                       \n\n    # Bottleneck (RCAM & others style)\n    b = e3\n    for _ in range(NUM_BOTTLENECK_BLOCKS):\n        b = bot_block(b, 256)\n\n    # Decoder\n    d3 = dec_block(b, e2, 128)                  \n    d2 = dec_block(d3, e1, 64)\n    d1 = dec_block(d2, s0_final, 64)\n\n    # Tail block\n    tail_refined = d1\n    for _ in range(NUM_TAIL_BLOCKS): \n        tail_refined = head_tail_block(tail_refined, filters=64)\n    \n    # Last upsample\n    x = layers.Conv2D(CH * (UPSCALE_FACTOR**2), 3, padding=\"same\")(tail_refined)     # Before upsample increase the number of channels \n    residual_output = PixelShuffle(block_size=UPSCALE_FACTOR)(x)                     # Upscale by Pixelshuffle \n    \n    # Global Residual Learning (FINISH)\n    unactivated_output = layers.Add(dtype=tf.float32)([upsampled_lr_base, residual_output])\n\n    # The final activation\n    outputs = layers.Activation(\"tanh\", dtype=tf.float32)(unactivated_output)\n\n    return keras.Model(inputs, outputs, name=\"UNet_SR_Generator\")\n\n# 3 Generator\ngenerator_u_net = build_unet_sr_generator()\ngenerator_u_net.summary()","metadata":{"_uuid":"752cffde-1af8-434f-9172-df63532c567e","_cell_guid":"0ebff147-5f5d-41eb-a082-3738d1ebcbb3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:45.359805Z","iopub.execute_input":"2025-09-25T18:25:45.359989Z","iopub.status.idle":"2025-09-25T18:25:47.631431Z","shell.execute_reply.started":"2025-09-25T18:25:45.359974Z","shell.execute_reply":"2025-09-25T18:25:47.630772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### PatchGAN\n\nInstead of classifying the entire image as real/fake (a single output), PatchGAN classifies N x N patches of the input image as real or fake.\nThis encourages the generator to produce realistic details across the entire image. The output is a single-channel feature map (e.g., 30x30x1) where each \"pixel\" corresponds to confidence that a specific patch of the input image is real. This single value is the discriminator's raw prediction (logit) about the \"realness\" of the patch of the original input image that corresponds to that location in the output grid.\n\nIn the output, there is **NO** activation function (like `sigmoid`). This is because the loss function, **Least Squares GAN (LSGAN)** loss, operates directly on the raw output values (logits). It penalizes the discriminator based on how far its predictions are from the target labels (1 for real, 0 for fake) using a mean squared error objective, which is known to provide more stable gradients than traditional cross-entropy.\n\n\"SpectralNormalization\" is used to increase stability by applying a spectral normalization on the weights of a target layer.","metadata":{"_uuid":"9de07536-76f7-4815-b21e-6ce72a6d0120","_cell_guid":"b548a124-0938-4b0b-a3b6-ab2f441bcdea","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 8.2 Discriminator (PatchGAN) Architecture \n\ndef build_discriminator(input_shape=(PATCH_H, PATCH_W, CH)):\n    # dtype=\"float32\" for stability\n    initializer = tf.random_normal_initializer(0., 0.02)\n    inputs = layers.Input(shape = input_shape, name = \"discriminator_input\", dtype = \"float32\")\n \n    # Block 1: C64\n    x = layers.Conv2D(64, 4, strides = 2, padding=  \"same\",\n                      kernel_initializer = initializer, use_bias = False, dtype = \"float32\")(inputs)\n    x = layers.LeakyReLU(0.2, dtype=\"float32\")(x)\n\n    # Block 2: C128\n\n    conv2 = layers.Conv2D(128, 4, strides = 2, padding=\"same\",\n                          kernel_initializer = initializer, use_bias = False, dtype = \"float32\")\n    x = SpectralNormalization(conv2, dtype = \"float32\")(x)\n    x = layers.LeakyReLU(0.2, dtype=\"float32\")(x)\n\n    # Block 3: C256\n    conv3 = layers.Conv2D(256, 4, strides = 2, padding = \"same\",\n                          kernel_initializer = initializer, use_bias = False, dtype = \"float32\")\n    x = SpectralNormalization(conv3, dtype = \"float32\")(x)\n    x = layers.LeakyReLU(0.2, dtype=\"float32\")(x)\n\n    # Block 4: C512\n    conv4 = layers.Conv2D(512, 4, strides = 1, padding = \"same\",\n                          kernel_initializer = initializer, use_bias = False, dtype = \"float32\")\n    x = SpectralNormalization(conv4, dtype = \"float32\")(x)\n    x = layers.LeakyReLU(0.2, dtype = \"float32\")(x)\n\n    # Output\n    conv_out = layers.Conv2D(1, 4, strides = 1, padding = \"same\",\n                             kernel_initializer = initializer, dtype = \"float32\")\n    patch_output = SpectralNormalization(conv_out, dtype = \"float32\")(x)\n\n    model = keras.Model(inputs, patch_output, name = \"PatchGAN_Discriminator_SN_float32\")\n\n    return model\n\n\ndiscriminator = build_discriminator()\ndiscriminator.summary()","metadata":{"_uuid":"8cfd558d-1484-418a-9e06-dcea1350da9c","_cell_guid":"c5e83c10-a1df-4fd1-bc89-f4cbcc22aac6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:47.632202Z","iopub.execute_input":"2025-09-25T18:25:47.632461Z","iopub.status.idle":"2025-09-25T18:25:47.70469Z","shell.execute_reply.started":"2025-09-25T18:25:47.632439Z","shell.execute_reply":"2025-09-25T18:25:47.703948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss Functions and Metrics\nLoss functions for models training:\n \n### Pixel and Perceptual Loss (for the Generator)\n *   **Mean Absolute Error (MAE):** A simple pixel-wise loss that measures the absolute difference between the predicted image and the ground truth. It's good for overall color and structure but can lead to blurry results.\n *   **Perceptual Loss:** This loss operates in a feature space, not pixel space. I use a pre-trained VGG19 network to extract features from both the predicted and ground truth images. By minimizing the MAE between these feature maps, the generator is encouraged to produce images that are *perceptually* similar to the ground truth, capturing high-level structures and textures more effectively.\n \n#### Adversarial Loss (for GAN training)\nInstead of using traditional Binary Cross-Entropy, this model uses the **Least Squares GAN (LSGAN)** loss.\n\n *   **Discriminator Loss:** The discriminator's goal is to output values close to 1 for real images and 0 for fake images. Its loss is the mean squared error between its predictions and these target values.\n\n *   **Generator GAN Loss:** The generator's goal is to fool the discriminator. To do this, it tries to generate images that the discriminator will classify as real (i.e. output a score of 1). Its loss is the mean squared error between the discriminator's predictions on fake images and a target of all ones.\n\n \n The generator's **total loss** in the GAN setup is a weighted sum of pixel loss, perceptual loss, and this adversarial loss.","metadata":{"_uuid":"b6b2feb8-f24a-4de7-8406-8d306d09cf64","_cell_guid":"820a3cad-c6f4-45c7-9bf9-9a41c668e814","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 9 Loss Functions\n\npixel_loss_fn = keras.losses.MeanAbsoluteError()\n\n# 9.1 Base: Pixel-By-Pixel loss (MAE)\ndef mae_only_loss(y_true, y_pred):\n    return pixel_loss_fn(y_true, y_pred)\n\n# 9.2 Advance: MAE + Perceptual (VGG19) loss\nvgg = VGG19(include_top = False, weights = \"imagenet\")   # \"include_top = FALSE\" to load VGG19 without classification layers\nvgg.trainable = False                                    # \"trainable = False\" to freeze its weight (I only want to extract feature)\n\n# 9.2 Extracion feature points\nvgg_layers = [\"block2_conv2\", \"block3_conv4\", \"block4_conv4\", \"block5_conv4\"]\noutputs = [vgg.get_layer(name).output for name in vgg_layers]\n\n# 9.2.1 Feature extraction \nfeat_ext_multi  = keras.Model(\n    inputs = vgg.input,\n    outputs = outputs       \n)\n\ndef perceptual_multi_layer(y_true, y_pred):              # y_true (ground truth) and y_pred (output of the models) are in range [-1, 1]\n    y_true_f32 = tf.cast(y_true, tf.float32)             # Cast inputs to float32 before passing to VGG.\n    y_pred_f32 = tf.cast(y_pred, tf.float32)             # Cast inputs to float32 before passing to VGG.\n    \n    y_true_0_255 = ((y_true_f32 + 1.0) / 2.0) * 255.0    # y_true is in [-1, 1]. They must be converted to [0, 255] for VGG.\n    y_pred_0_255 = ((y_pred_f32 + 1.0) / 2.0) * 255.0    # y_pred is in [-1, 1]. They must be converted to [0, 255] for VGG.\n   \n    yt_vgg = preprocess_input(y_true_0_255)              # Denormalize to [0, 255] to enable \"preprocess_input\" function\n    yp_vgg = preprocess_input(y_pred_0_255)\n    features_true = feat_ext_multi (yt_vgg)              # Extract feature from VGG19 for true and predicted images\n    features_pred = feat_ext_multi (yp_vgg) \n    \n    # Calculate the loss for each layer and add them up\n    total_perceptual_loss = 0.0\n    layer_weights = [0.303, 0.303, 0.242, 0.151]                # More weight to deeper features\n\n    for i in range(len(features_true)):\n        layer_loss = pixel_loss_fn(features_true[i], features_pred[i])     # Use Mae for layer loss of feature extracted\n        total_perceptual_loss += layer_loss * layer_weights[i]             # Multiply each layer for it's weight\n\n    return total_perceptual_loss\n\n\n# 9.3 Combined loss MAE + Perceptual Loss\ndef content_loss_perceptual(y_true, y_pred):\n    pixel_l = pixel_loss_fn(y_true, y_pred)\n    perceptual_l = perceptual_multi_layer(y_true, y_pred)\n    return pixel_l + PERCEPTUAL_WEIGHT_PRE * perceptual_l","metadata":{"_uuid":"2031313b-b2fa-42f7-8708-e8b6e2fa94fa","_cell_guid":"7b5a603d-ac2e-4942-bc5c-ec0c37ae6e40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:47.705506Z","iopub.execute_input":"2025-09-25T18:25:47.705704Z","iopub.status.idle":"2025-09-25T18:25:48.052476Z","shell.execute_reply.started":"2025-09-25T18:25:47.705681Z","shell.execute_reply":"2025-09-25T18:25:48.051695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 9.4 Adversarial (GAN) Loss functions \n\n# 9.4.1 Least Squares GAN (LSGAN) Loss\n# Instead of using Binary Cross-Entropy (which can lead to vanishing gradients when the\n# discriminator becomes too confident), we use the Least Squares loss. This loss function\n# penalizes predictions that are far from the target label (1 for real, 0 for fake)\n# using a Mean Squared Error objective.\n\n# 9.4.2 Discriminator Loss (LSGAN)\n\ndef discriminator_loss(real_output, fake_output):\n    \"\"\"\n    Calculates the Least Squares loss for the discriminator.\n    The discriminator's goal is to output values close to 1 for real images\n    and close to 0 for fake images. We use Mean Squared Error for this.\n    \"\"\"\n    \n    real_output = tf.cast(real_output, tf.float32)\n    fake_output = tf.cast(fake_output, tf.float32)\n    \n    # Loss for real images: Measures how far the discriminator's predictions\n    # are from the target of 1.0. The use of 0.9 is for label smoothing\n    real_loss = tf.reduce_mean(tf.square(real_output - 0.9))\n\n    # Loss for fake images: Measures how far the discriminator's predictions\n    # are from the target of 0.0.\n    fake_loss = tf.reduce_mean(tf.square(fake_output))\n\n    # The total loss is the average of the two\n    return 0.5 * (real_loss + fake_loss)\n \n\n# 9.4.3 Generator loss\n\ndef generator_gan_loss(fake_output):\n    \"\"\"\n    Calculates the Least Squares loss for the generator.\n    The generator's goal is to fool the discriminator. It wants the discriminator\n    to score its fake images as 1.0 (real). This loss measures how far the\n    discriminator's predictions for the fake images are from this target.\n    \"\"\"\n    fake_output = tf.cast(fake_output, tf.float32)\n    \n    # By minimizing this loss, the generator learns to produce images that\n    # the discriminator scores as close to 1.0 as possible\n    return tf.reduce_mean(tf.square(fake_output - 1.0))","metadata":{"_uuid":"2ef650db-c45a-4f04-83f1-6fb2b6398e63","_cell_guid":"6601a0ec-3253-4de9-aec6-a6dd64f44624","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.053382Z","iopub.execute_input":"2025-09-25T18:25:48.053662Z","iopub.status.idle":"2025-09-25T18:25:48.059087Z","shell.execute_reply.started":"2025-09-25T18:25:48.053641Z","shell.execute_reply":"2025-09-25T18:25:48.05836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ## SRRGAN Custom Model\nTo handle the two training part process of a GAN (updating the discriminator and generator separately), a custom `keras.Model` subclass is created that was made compatible with Keras APIs like `.fit()` and callbacks.\n \n### Training Step Logic:\n1.  **Train Discriminator:**\n    1.   Generate a batch of fake (SR) images using the generator;\n    2.   The discriminator makes predictions for both real (HR) and fake (SR) images.\n    3.   Calculate the `discriminator_loss`.\n    4.   Compute gradients and update ONLY the discriminator's weights.\n \n2.  **Train Generator:**\n     1.   Generate a new batch of fake images *inside a new gradient tape*.\n     2.   Get the discriminator's verdict on these fake images.\n     3.   Calculate the generator's total loss: a combination of `content_loss_perceptual` (pixel + VGG) and `generator_gan_loss`.\n     4.   Compute gradients and update only the generator's weights.\n \n3.  **Update Metrics:** Update and log all relevant metrics (losses, PSNR, SSIM).","metadata":{"_uuid":"bedd966c-3357-46d1-9abd-3ae57e6532ee","_cell_guid":"e06917b9-0a3a-4091-b3ab-c7252e647ec4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 9.5 SRRGAN Custom Model Class \n\n@register_keras_serializable()       # Allows Keras to save and load the custom model\nclass SRRGAN(keras.Model):\n    \"\"\"\n    Custom Keras Model that encapsulates the SRRGAN training logic.\n    This class  implement the two-part update rule for GANs (updating discriminator and generator separately).\n    It also handles metric tracking and serialization.\n    \"\"\"\n    def __init__(self, generator, discriminator, g_optimizer, d_optimizer,\n                 pixel_weight = PIXEL_WEIGHT_GAN, gan_weight = GLOBAL_GAN_WEIGHT , perceptual_weight = PERCEPTUAL_WEIGHT_GAN, **kwargs):\n        super().__init__(**kwargs)\n        \n        # --- Core Components ---\n        # The SRRGAN model holds the generator, discriminator, and their optimizers\n        # as internal attributes.\n        self.generator = generator\n        self.discriminator = discriminator\n        self.g_optimizer = g_optimizer\n        self.d_optimizer = d_optimizer\n        \n        # --- Hyperparameters ---\n        # Weights for the different components of the generator's total loss.\n        self.pixel_weight = PIXEL_WEIGHT_GAN\n        self.gan_weight = gan_weight                 \n        self.perceptual_weight = perceptual_weight\n\n        # --- Metric Trackers ---\n        # Keras metrics to track the mean of different loss values over each epoch.\n        # Raw losses\n        self.g_total_loss_tracker = metrics.Mean(name=\"g_total_loss\")\n        self.g_pixel_loss_tracker = metrics.Mean(name=\"g_pixel_loss\")\n        self.g_perceptual_loss_tracker = metrics.Mean(name=\"g_perceptual_loss\")\n        self.g_gan_loss_tracker = metrics.Mean(name=\"g_gan_loss\")\n        self.d_loss_tracker = metrics.Mean(name=\"d_loss\")\n        \n        # Weighted losses\n        self.g_pixel_loss_weighted_tracker = metrics.Mean(name=\"g_pixel_loss_weighted\")\n        self.g_perceptual_loss_weighted_tracker = metrics.Mean(name=\"g_perceptual_loss_weighted\")\n        self.g_gan_loss_weighted_tracker = metrics.Mean(name=\"g_gan_loss_weighted\")\n\n        # standard metrics\n        self.psnr_tracker = metrics.Mean(name=\"psnr\")\n        self.ssim_tracker = metrics.Mean(name=\"ssim\")\n\n    def get_config(self):\n        \"\"\"\n        Enables the model to be saved and loaded.\n        This method returns a dictionary containing the configurations of all\n        its components, which Keras uses during serialization.\n        \"\"\"\n        base = super().get_config()\n        base.update({\n            # \"serialize_keras_object\" converts the models and optimizers into a\n            # savable dictionary format.\n            \"generator\": keras.saving.serialize_keras_object(self.generator),\n            \"discriminator\": keras.saving.serialize_keras_object(self.discriminator),\n            \"g_optimizer\": keras.saving.serialize_keras_object(self.g_optimizer),\n            \"d_optimizer\": keras.saving.serialize_keras_object(self.d_optimizer),\n            \"pixel_weight\": float(self.pixel_weight),\n            \"gan_weight\": float(self.gan_weight),\n            \"perceptual_weight\": float(self.perceptual_weight),\n        })\n        return base\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"\n        Creates an SRRGAN model instance from a configuration dictionary.\n        This method is the counterpart to `get_config`, used by Keras to\n        reconstruct the model when loading it from a file.\n        \"\"\"\n        # \"deserialize_keras_object\" reconstructs the Python objects from their\n        # dictionary representations.\n        generator = keras.saving.deserialize_keras_object(config.pop(\"generator\"))\n        discriminator = keras.saving.deserialize_keras_object(config.pop(\"discriminator\"))\n        g_optimizer = keras.saving.deserialize_keras_object(config.pop(\"g_optimizer\"))\n        d_optimizer = keras.saving.deserialize_keras_object(config.pop(\"d_optimizer\"))\n\n        # The remaining items in the config are passed to the class constructor.\n        return cls(\n            generator=generator,\n            discriminator=discriminator,\n            g_optimizer=g_optimizer,\n            d_optimizer=d_optimizer,\n            **config\n        )\n    \n    \n    @property\n    def metrics(self):\n        return [\n            self.g_total_loss_tracker, \n            self.d_loss_tracker,\n            # Raw losses\n            self.g_pixel_loss_tracker,\n            self.g_perceptual_loss_tracker, \n            self.g_gan_loss_tracker,\n            # Weighted losses\n            self.g_pixel_loss_weighted_tracker,\n            self.g_perceptual_loss_weighted_tracker,\n            self.g_gan_loss_weighted_tracker,\n            # Standard metrics\n            self.psnr_tracker, \n            self.ssim_tracker,\n        ]\n\n    def train_step(self, data):\n        \"\"\"\n        Defines the logic for one training step (one batch of data).\n        \"\"\"\n        # Unpack the data.\n        lr_images, hr_images = data\n\n        # --- 1. Train the Discriminator ---\n        # GradientTape to record operations for automatic differentiation.\n        with tf.GradientTape() as tape:\n            # Generate a batch of fake images.\n            sr_images = self.generator(lr_images, training=True)            \n            # Get the discriminator's predictions for both real and fake images.\n            real_output = self.discriminator(hr_images, training=True)\n            fake_output = self.discriminator(sr_images, training=True)\n            # Calculate the discriminator's loss.\n            d_loss = discriminator_loss(real_output, fake_output)\n        \n        # Compute the gradients of the loss with respect to the discriminator's weights.\n        d_grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n        # Apply the gradients to update the discriminator's weights.\n        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n\n        # --- 2. Train the Generator ---\n        with tf.GradientTape() as tape:\n            # Generate a batch of fake images.\n            sr_images = self.generator(lr_images, training=True)\n            # Get the discriminator's predictions\n            fake_output = self.discriminator(sr_images, training=True)\n            \n            # Calculate raw loss components.            \n            pixel_l = pixel_loss_fn(hr_images, sr_images)\n            perceptual_l = perceptual_multi_layer(hr_images, sr_images)\n            gan_l = generator_gan_loss(fake_output)\n\n            # Calculated weight loss components\n            pixel_l_w = self.pixel_weight * tf.cast(pixel_l, tf.float32)\n            perceptual_l_w = self.perceptual_weight * tf.cast(perceptual_l, tf.float32)\n            gan_l_w = self.gan_weight * tf.cast(gan_l, tf.float32)\n\n            # Combine the weighted losses into the total loss        \n            g_total_loss = pixel_l_w + perceptual_l_w + gan_l_w\n            \n\n        # Compute gradients and update the generator's weights.\n        g_grads = tape.gradient(g_total_loss, self.generator.trainable_variables)\n        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n\n        # --- 3. Update Metrics ---\n        self.d_loss_tracker.update_state(d_loss)\n        self.g_total_loss_tracker.update_state(g_total_loss)\n        \n        # Update raw loss \n        self.g_pixel_loss_tracker.update_state(pixel_l)\n        self.g_perceptual_loss_tracker.update_state(perceptual_l)\n        self.g_gan_loss_tracker.update_state(gan_l)\n        \n        # Update weighted\n        self.g_pixel_loss_weighted_tracker.update_state(pixel_l_w)\n        self.g_perceptual_loss_weighted_tracker.update_state(perceptual_l_w)\n        self.g_gan_loss_weighted_tracker.update_state(gan_l_w)\n        \n        # Update standard metrics\n        self.psnr_tracker.update_state(psnr_metric(hr_images, sr_images))\n        self.ssim_tracker.update_state(ssim_metric(hr_images, sr_images))\n\n        # Return a dictionary of the current metric values. Keras displays this.\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        \"\"\"\n        Logic of one evaluation step.\n        In the test step,  the evaluation is only on the generator's performance.\n        The discriminator is not involved.\n        \"\"\"\n        lr_images, hr_images = data\n        \n        # Generate SR images in inference mode.\n        sr_images = self.generator(lr_images, training=False)\n        # Update evaluation metrics.\n        self.psnr_tracker.update_state(psnr_metric(hr_images, sr_images))\n        self.ssim_tracker.update_state(ssim_metric(hr_images, sr_images))\n        pixel_l = pixel_loss_fn(hr_images, sr_images)\n        perceptual_l = perceptual_multi_layer(hr_images, sr_images)\n        self.g_pixel_loss_tracker.update_state(pixel_l)\n        self.g_perceptual_loss_tracker.update_state(perceptual_l)\n\n        # Return a dictionary of the final evaluation metrics.\n        return {\n            \"psnr\": self.psnr_tracker.result(),\n            \"ssim\": self.ssim_tracker.result(),\n            \"pixel_loss\": self.g_pixel_loss_tracker.result(),\n            \"perceptual_loss\": self.g_perceptual_loss_tracker.result()\n        }","metadata":{"_uuid":"abc4a69b-bfc4-4214-b979-5d749144ccf2","_cell_guid":"541fa2ab-40d2-41d1-a2c5-196f18e5c14b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.059869Z","iopub.execute_input":"2025-09-25T18:25:48.06008Z","iopub.status.idle":"2025-09-25T18:25:48.079051Z","shell.execute_reply.started":"2025-09-25T18:25:48.060065Z","shell.execute_reply":"2025-09-25T18:25:48.078321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training\nThis section compiles and trains the models with our different loss function configurations.\n \n ### Training Strategy:\nA three-stage training strategy is applied:\n1.  **MAE Only:** Train a generator using only MAE loss. This provides a baseline and establishes a good initial weight configuration.\n2.  **MAE + Perceptual:** Continue to Pre-train the generator by adding the Perceptual part. This serves as a strong pre-trained model for the final GAN step.\n3.  **SRRGAN (Fine-tuning):** Initialize the generator with the weights from the MAE + Perceptual model. Then, train it adversarially against the discriminator.","metadata":{"_uuid":"f537444f-2517-4448-bca8-30cf7d60a6b2","_cell_guid":"c90a47e2-1454-4b52-a057-7ba910a1b581","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 10 Additional Metrics\n\ndef denorm_for_metric(tensor):          # Denormalize from [-1, 1] to [0, 1] for standard metric calculation.\n    return (tensor + 1.0) / 2.0\n\ndef psnr_metric(y_true, y_pred):\n    return tf.image.psnr(denorm_for_metric(y_true), denorm_for_metric(y_pred), max_val=1.0)\n\ndef ssim_metric(y_true, y_pred):\n    return tf.image.ssim(denorm_for_metric(y_true), denorm_for_metric(y_pred), max_val=1.0)","metadata":{"_uuid":"29b900ce-94f3-4f0b-ab85-4ae7717945d9","_cell_guid":"3f047dd2-b0aa-41f1-addf-4f144ad6b230","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.079743Z","iopub.execute_input":"2025-09-25T18:25:48.079982Z","iopub.status.idle":"2025-09-25T18:25:48.096146Z","shell.execute_reply.started":"2025-09-25T18:25:48.079958Z","shell.execute_reply":"2025-09-25T18:25:48.095457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 11.1 Standard Plots\n\ndef plot_standard_metrics(history, title_prefix=\"\"):\n    epochs = range(1, len(history.history[\"loss\"]) + 1)\n    \n    plt.figure(figsize=(15, 4))\n\n    # Loss\n    plt.subplot(1, 3, 1)\n    plt.plot(epochs, history.history[\"loss\"], label=\"Train Loss\")\n    plt.plot(epochs, history.history[\"val_loss\"], label=\"Val Loss\")\n    plt.title(f\"{title_prefix} Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss Value\")\n    plt.legend()\n\n    # PSNR\n    plt.subplot(1, 3, 2)\n    plt.plot(epochs, history.history[\"psnr_metric\"], label=\"Train PSNR\")\n    plt.plot(epochs, history.history[\"val_psnr_metric\"], label=\"Val PSNR\")\n    plt.title(f\"{title_prefix} PSNR\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"PSNR (dB)\")\n    plt.legend()\n\n    # SSIM\n    plt.subplot(1, 3, 3)\n    plt.plot(epochs, history.history[\"ssim_metric\"], label=\"Train SSIM\")\n    plt.plot(epochs, history.history[\"val_ssim_metric\"], label=\"Val SSIM\")\n    plt.title(f\"{title_prefix} SSIM\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"SSIM\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n\n# --- 11.2 GAN Plots\n\ndef plot_gan_metrics(history, title_prefix=\"SRRGAN\"):\n    epochs = range(1, len(history.history[\"g_total_loss\"]) + 1)\n\n    plt.figure(figsize=(18, 8))\n\n    # Weighted loss components\n    plt.subplot(2, 2, 1)\n    plt.plot(epochs, history.history[\"g_total_loss\"], label=\"Total Gen. Loss\", linewidth=2)\n    plt.plot(epochs, history.history[\"g_pixel_loss_weighted\"], label=\"Weighted Pixel Loss\", linestyle='--')\n    plt.plot(epochs, history.history[\"g_perceptual_loss_weighted\"], label=\"Weighted Perceptual Loss\", linestyle='-.')\n    plt.plot(epochs, history.history[\"g_gan_loss_weighted\"], label=\"Weighted GAN Loss\", linestyle=':')\n    plt.title(f\"{title_prefix} Weighted Generator Loss Components (Train)\") \n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss Value\")\n    plt.legend()\n\n    # Discriminator Loss\n    plt.subplot(2, 2, 2)\n    plt.plot(epochs, history.history[\"d_loss\"], label=\"Disc. Loss\", color='red')\n    plt.title(f\"{title_prefix} Discriminator Loss (Train)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    # PSNR\n    plt.subplot(2, 2, 3)\n    plt.plot(epochs, history.history[\"psnr\"], label=\"Train PSNR\")\n    plt.plot(epochs, history.history[\"val_psnr\"], label=\"Val PSNR\")\n    plt.title(f\"{title_prefix} PSNR\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"PSNR (dB)\")\n    plt.legend()\n\n    # SSIM\n    plt.subplot(2, 2, 4)\n    plt.plot(epochs, history.history[\"ssim\"], label=\"Train SSIM\")\n    plt.plot(epochs, history.history[\"val_ssim\"], label=\"Val SSIM\")\n    plt.title(f\"{title_prefix} SSIM\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"SSIM\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"119854be-c213-4b94-bb1f-b275424deed5","_cell_guid":"dd6dc18d-09d2-4dbe-98a0-b24ac061a9d1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.096993Z","iopub.execute_input":"2025-09-25T18:25:48.097231Z","iopub.status.idle":"2025-09-25T18:25:48.113913Z","shell.execute_reply.started":"2025-09-25T18:25:48.097209Z","shell.execute_reply":"2025-09-25T18:25:48.113371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 12.1 Callbacks\n\ndef callbacks_mae (checkpoint_path, patience_es = 15, patience_lr = 5):\n    return [\n        keras.callbacks.ModelCheckpoint(                                        # Save only the model with the best validation loss\n            filepath = checkpoint_path,\n            save_best_only = True,\n            monitor = \"val_loss\",\n            verbose = 1\n        ),\n        keras.callbacks.EarlyStopping(                                          # Stop training if there are not improvement\n            patience = patience_es,\n            restore_best_weights = True,\n            monitor = \"val_loss\",\n            verbose = 1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor = 'val_loss', \n            factor = 0.5,        \n            patience = patience_lr, \n            verbose = 1,\n            min_lr = 1e-6         \n        )\n    ]\n\ndef callbacks_perceptual  (checkpoint_path, patience_es = 16, patience_lr = 4):\n    return [\n        keras.callbacks.ModelCheckpoint(                                        # Save only the model with the best validation loss\n            filepath = checkpoint_path,\n            save_best_only = True,\n            monitor = \"val_loss\",\n            verbose = 1\n        ),\n        keras.callbacks.EarlyStopping(                                          # Stop training if there are not improvement\n            patience = patience_es,\n            restore_best_weights = True,\n            monitor = \"val_loss\",\n            verbose = 1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor = 'val_loss', \n            factor = 0.5,        \n            patience = patience_lr, \n            verbose = 1,\n            min_lr = 5e-8         \n        )\n    ]\n\n\n# 12.2 Checkpoint Position\nCHECKPOINT_MAE        = \"checkpoint_mae.keras\"\nCHECKPOINT_PERCEPTUAL = \"checkpoint_perceptual.keras\"\n\n\n# 12.3 callbacks creation\ncallbacks_mae        = callbacks_mae(CHECKPOINT_MAE)\ncallbacks_perceptual = callbacks_perceptual(CHECKPOINT_PERCEPTUAL)\n\n\n# --- 12.4 GAN callbacks\n\nsteps_per_epoch = len(train_ds)\n\ngan_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath = \"gan_checkpoints/SRRGAN_epoch_{epoch:02d}.keras\",\n    save_weights_only = False,\n    save_best_only = False,               # Save PERIODICALLY, not just the best\n    save_freq = steps_per_epoch * 4       # Save every 4 epochs\n)\n\n\nclass GANMonitor(keras.callbacks.Callback):\n    \"\"\"\n    Callback to generate and show images \n    \"\"\"\n    def __init__(self, val_dataset, num_samples=3, frequency=1, log_dir=\"gan_images\"):\n        super().__init__()\n        val_batch = next(iter(val_dataset))\n        self.lr_images = val_batch[0][:num_samples]\n        self.hr_images = val_batch[1][:num_samples]\n        self.num_samples = num_samples\n        self.frequency = frequency  \n        \n        self.log_dir = log_dir\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n\n    def denorm(self, img):\n        img = tf.cast(img, tf.float32)\n        return tf.clip_by_value((img + 1.0) / 2.0, 0, 1)\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Check if the current epoch is a multiple of the frequency\n        # The `+ 1` is because epochs are 0-indexed in the callback.\n        if (epoch + 1) % self.frequency == 0:\n            generator = self.model.generator\n            sr_images_pred = generator.predict(self.lr_images, verbose=0)\n\n            print(f\"\\n--- Generating images for epoch {epoch + 1} ---\")\n            for i in range(self.num_samples):\n                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n                \n                axes[0].imshow(self.denorm(self.lr_images[i]))\n                axes[0].set_title(\"Low-Res Input\")\n                axes[0].axis(\"off\")\n                \n                axes[1].imshow(self.denorm(sr_images_pred[i]))\n                axes[1].set_title(\"Super-Res (Generated)\")\n                axes[1].axis(\"off\")\n\n                axes[2].imshow(self.denorm(self.hr_images[i]))\n                axes[2].set_title(\"High-Res (Ground Truth)\")\n                axes[2].axis(\"off\")\n\n                fig.suptitle(f'Epoch: {epoch + 1}', fontsize=16)\n                \n                filepath = os.path.join(self.log_dir, f'epoch_{epoch+1:03d}_sample_{i+1}.png')\n                plt.savefig(filepath)\n                \n                plt.show()\n                plt.close(fig)","metadata":{"_uuid":"9a905344-917c-4562-8ce9-9a195d5e206e","_cell_guid":"80e0b836-3f49-426d-8bd4-82287902ae06","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.114824Z","iopub.execute_input":"2025-09-25T18:25:48.11508Z","iopub.status.idle":"2025-09-25T18:25:48.131237Z","shell.execute_reply.started":"2025-09-25T18:25:48.115059Z","shell.execute_reply":"2025-09-25T18:25:48.130742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 13 Optimizers: Schedule, mixedprecision & dictionary\n\n# --- Mae & Mae + Perceptual\n\n# 13.1 Learning rate\ninitial_lr_mae = 5e-4\ninitial_lr_perceptual = 1e-5\n\n# 13.2 Adam optimizer \noptimizer_mae = keras.optimizers.Adam(\n    learning_rate = initial_lr_mae,\n    weight_decay = 1e-4\n)\n\noptimizer_perceptual = keras.optimizers.Adam(\n    learning_rate = initial_lr_perceptual,\n    weight_decay = 5e-5\n)\n\n\n# 13.3 Add Mixed precision\n\noptimizer_mae = tf.keras.mixed_precision.LossScaleOptimizer(optimizer_mae)\noptimizer_perceptual = tf.keras.mixed_precision.LossScaleOptimizer(optimizer_perceptual)\n\n\n\n\n\n# --- GAN networks \n\n# Number of training steps\nsteps_per_epoch = len(train_ds)\ntotal_steps = steps_per_epoch * EPOCHS \n\n# Schedule for generator and discriminator\ng_lr_schedule = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate = 5e-5,   # Generator LR\n    decay_steps = total_steps,\n    alpha=0.0\n)\n\nd_lr_schedule = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate = 2.5e-5,  # Discriminator LR \n    decay_steps = total_steps,\n    alpha = 0.0\n)\n\n\n# Optimizers with schedule. One for the generator and one for the discriminator\ng_optimizer_gan = keras.optimizers.Adam(learning_rate = g_lr_schedule,\n                                        beta_1 = 0.5,\n                                        clipnorm = 1.0\n                                       )\n\nd_optimizer_gan = keras.optimizers.Adam(learning_rate = d_lr_schedule,\n                                        beta_1 = 0.5,\n                                        clipnorm=1.0\n                                       )\n\n\n# --- 13.5 Custom objects\n\n# Define custom objects (so Keras knows how to load the model)\ncustom_objects = {\n    \"mae_only_loss\": mae_only_loss,\n    \"content_loss_perceptual\": content_loss_perceptual,\n    \"perceptual_multi_layer\" : perceptual_multi_layer,\n    \"pixel_loss_fn\": pixel_loss_fn,\n    \"psnr_metric\": psnr_metric,\n    \"ssim_metric\": ssim_metric,\n    \"PixelShuffle\": PixelShuffle,\n    \"SRRGAN\": SRRGAN,\n    \"SpatialAttentionBlock\": SpatialAttentionBlock\n}","metadata":{"_uuid":"7335285b-d1c4-4578-b857-6b96e65d0db6","_cell_guid":"64fe9188-46b5-4fad-994f-350d9881060b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-25T18:25:48.132031Z","iopub.execute_input":"2025-09-25T18:25:48.132631Z","iopub.status.idle":"2025-09-25T18:25:48.160364Z","shell.execute_reply.started":"2025-09-25T18:25:48.132605Z","shell.execute_reply":"2025-09-25T18:25:48.159689Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MAE --- Baseline","metadata":{"_uuid":"d79a7d5f-315c-4498-a362-0e88573b9a15","_cell_guid":"23d71897-9720-4cf7-a533-d24da8f8bf09","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 14.1 Compile U-Net with MAE\n\ngenerator_u_net.compile(\n    optimizer = optimizer_mae,    \n    loss = mae_only_loss,\n    metrics = [\n        psnr_metric,\n        ssim_metric,\n        mae_only_loss\n    ]\n)","metadata":{"_uuid":"375536b6-0478-4e90-8a0a-e030e450afc9","_cell_guid":"2eb781f3-4f20-405b-8e89-fdd449e4f444","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.16109Z","iopub.execute_input":"2025-09-25T18:25:48.16159Z","iopub.status.idle":"2025-09-25T18:25:48.168913Z","shell.execute_reply.started":"2025-09-25T18:25:48.161563Z","shell.execute_reply":"2025-09-25T18:25:48.1682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 14.2 Train U-Net with MAE (Training was done in a previouse Version)\n\n#csv_logger_mae = CSVLogger(\"history_log_mae.csv\")\n\n#history_mae  = generator_u_net.fit(train_ds,\n#                              validation_data = val_ds,\n#                              epochs = EPOCHS,\n#                              callbacks = callbacks_mae + [csv_logger_mae]\n#                                )","metadata":{"_uuid":"3a7e0aae-594f-4fe4-aa5d-5ddeb937f0b2","_cell_guid":"aa1bafe1-c7b2-46cf-a50c-36ec46101070","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.169572Z","iopub.execute_input":"2025-09-25T18:25:48.169751Z","iopub.status.idle":"2025-09-25T18:25:48.178144Z","shell.execute_reply.started":"2025-09-25T18:25:48.169736Z","shell.execute_reply":"2025-09-25T18:25:48.177492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 14.3 Plot Train vs Val\n\n# Load the history from the CSV file\nhistory_df_mae = pd.read_csv(\"/kaggle/input/history-mae-and-mae-perceptual/history_log_mae.csv\")\n\nmock_history_mae = SimpleNamespace(history = history_df_mae.to_dict('list'))\n\n# Plot\nplot_standard_metrics(mock_history_mae, title_prefix=\"U-Net with MAE\")","metadata":{"_uuid":"f5093339-d5cd-451e-b8b9-1f7cab6798bb","_cell_guid":"81c6c6fb-adc3-4a48-aca7-4a1baf25b7dc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.178955Z","iopub.execute_input":"2025-09-25T18:25:48.179192Z","iopub.status.idle":"2025-09-25T18:25:48.681606Z","shell.execute_reply.started":"2025-09-25T18:25:48.179171Z","shell.execute_reply":"2025-09-25T18:25:48.680881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 14.4 Save U-Net with MAE (Save was done in Version \"Pre-Train Mae & Mae+Perceptual\")\n\n#generator_u_net.save(\"model_mae.keras\")","metadata":{"_uuid":"210e22c7-9006-45b3-b1fc-147e7579e2bf","_cell_guid":"3af33037-7264-4f72-9640-cd1b47a5518f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:48.682259Z","iopub.execute_input":"2025-09-25T18:25:48.682466Z","iopub.status.idle":"2025-09-25T18:25:48.68582Z","shell.execute_reply.started":"2025-09-25T18:25:48.68245Z","shell.execute_reply":"2025-09-25T18:25:48.685062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MAE + perceptual --- & Pre-Training","metadata":{"_uuid":"edf63245-e732-42ea-8088-d3b5c7809e32","_cell_guid":"c0a98519-81e1-4d67-9382-5186d2b7c858","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 15.1 Continue pre-training\n\n#generator_u_net = build_unet_sr_generator()\n#generator_u_net = load_model(\"/kaggle/input/model_mae/keras/default/1/model_mae.keras\", custom_objects=custom_objects)","metadata":{"_uuid":"b87d5da5-c378-4996-860f-acfa1bb5af7f","_cell_guid":"71131252-bbb1-45ae-a844-2ce9c2b8a97b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-25T18:25:48.686505Z","iopub.execute_input":"2025-09-25T18:25:48.686703Z","iopub.status.idle":"2025-09-25T18:25:48.697697Z","shell.execute_reply.started":"2025-09-25T18:25:48.686689Z","shell.execute_reply":"2025-09-25T18:25:48.697206Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 15.2 Compile U-Net with MAE + Perceptual\n\ngenerator_u_net.compile(\n    optimizer = optimizer_perceptual,     \n    loss = content_loss_perceptual,\n    metrics = [\n        perceptual_multi_layer,\n        pixel_loss_fn,\n        psnr_metric,\n        ssim_metric\n    ]\n)","metadata":{"_uuid":"efc55204-9950-4ff7-baa4-8c056eea486b","_cell_guid":"e54ec2a6-0758-479e-bdb4-f0fd7a01826a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-25T18:25:48.698571Z","iopub.execute_input":"2025-09-25T18:25:48.698844Z","iopub.status.idle":"2025-09-25T18:25:48.712496Z","shell.execute_reply.started":"2025-09-25T18:25:48.698822Z","shell.execute_reply":"2025-09-25T18:25:48.711978Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 15.3 Train U-Net with MAE + Perceptual (Training was done in a previouse Version)\n\n#csv_logger_perceptual = CSVLogger(\"history_log_perceptual.csv\")\n\n#history_perceptual  = generator_u_net.fit(train_ds,\n#                                  validation_data = val_ds,\n#                                  epochs = EPOCHS,\n#                                  callbacks = callbacks_perceptual + [csv_logger_perceptual]\n#                                         )","metadata":{"_uuid":"88bf9563-4945-4ac3-8c08-02f2fd115c3a","_cell_guid":"b7a1e792-87f6-4a3f-84f5-594db9cfd4cb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-25T18:25:48.715852Z","iopub.execute_input":"2025-09-25T18:25:48.71607Z","iopub.status.idle":"2025-09-25T18:25:48.722496Z","shell.execute_reply.started":"2025-09-25T18:25:48.716043Z","shell.execute_reply":"2025-09-25T18:25:48.721891Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 15.4 Plot Train vs Val\n\n# Load the history from the CSV file\nhistory_df_perceptual = pd.read_csv(\"/kaggle/input/history-mae-and-mae-perceptual/history_log_perceptual.csv\")\n\nmock_history_perceptual = SimpleNamespace(history = history_df_perceptual.to_dict('list'))\n\n# Plot\nplot_standard_metrics(mock_history_perceptual, title_prefix = \"U-Net with Mae + Perceptual Loss\")","metadata":{"_uuid":"3256b609-5bab-4a0f-88f8-d5860e1063df","_cell_guid":"1e5ccf5a-1f16-44aa-bd09-ce2f96f8e238","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-25T18:25:48.723161Z","iopub.execute_input":"2025-09-25T18:25:48.723375Z","iopub.status.idle":"2025-09-25T18:25:49.243156Z","shell.execute_reply.started":"2025-09-25T18:25:48.723331Z","shell.execute_reply":"2025-09-25T18:25:49.242382Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 15.5 Save U-Net with MAE + Perceptual (Save was done in Version \"Pre-Train Mae & Mae+Perceptual\")\n\n#generator_u_net.save(\"model_perceptual.keras\")","metadata":{"_uuid":"f0618cf3-6548-4692-b761-2dfd2a6bbef2","_cell_guid":"ab5d9277-f2ae-4b96-baf7-690279d33b06","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:49.243965Z","iopub.execute_input":"2025-09-25T18:25:49.244171Z","iopub.status.idle":"2025-09-25T18:25:49.247522Z","shell.execute_reply.started":"2025-09-25T18:25:49.244155Z","shell.execute_reply":"2025-09-25T18:25:49.246819Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MAE + perceptual + GAN --- Fine-Tuning","metadata":{"_uuid":"d45d444e-8b65-4162-ae9f-7b2bdfbfb6ae","_cell_guid":"9d1ca90e-bde2-4650-8e21-9fd10bfd12b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 16.1 Fine-Tune\n\ngenerator_u_net = build_unet_sr_generator()\ngenerator_u_net = load_model(\"/kaggle/input/model-mae-and-maeperceptual/keras/default/1/model_perceptual.keras\", custom_objects=custom_objects)","metadata":{"_uuid":"097aa57c-0bbe-4c17-b66d-0a193ea193cb","_cell_guid":"8d113730-b768-4544-9f1f-36699708e9bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:49.248399Z","iopub.execute_input":"2025-09-25T18:25:49.248676Z","iopub.status.idle":"2025-09-25T18:25:52.910415Z","shell.execute_reply.started":"2025-09-25T18:25:49.248653Z","shell.execute_reply":"2025-09-25T18:25:52.909803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 16.2 Compile SRRGAN \n  \nSRRGAN = SRRGAN(\n    generator = generator_u_net, \n    discriminator = discriminator,\n    g_optimizer = g_optimizer_gan,\n    d_optimizer = d_optimizer_gan\n)\n\nSRRGAN.compile(optimizer=g_optimizer_gan)","metadata":{"_uuid":"babea828-e121-4e3f-aba0-3bfd1fff69bc","_cell_guid":"a2df349b-bef8-4745-bf01-cb75a6d9ff0e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:52.911293Z","iopub.execute_input":"2025-09-25T18:25:52.911524Z","iopub.status.idle":"2025-09-25T18:25:52.949471Z","shell.execute_reply.started":"2025-09-25T18:25:52.911509Z","shell.execute_reply":"2025-09-25T18:25:52.948781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 16.3 Train SRRGAN  \n\n# Images visualization (frequency = after how many epochs new generation)\ngan_monitor_callback = GANMonitor(val_dataset=val_ds, num_samples=3, frequency=5)\n\ncsv_logger_gan = CSVLogger(\"history_log_gan.csv\")\n\nhistory_gan = SRRGAN.fit(\n    train_ds,\n    validation_data = val_ds,\n    epochs = EPOCHS,\n    callbacks = [gan_checkpoint_callback, gan_monitor_callback, csv_logger_gan]\n)","metadata":{"_uuid":"8177d272-1718-44dd-b73d-34332f8eb280","_cell_guid":"ddf3454c-41f1-4947-a5e7-c6bc942f06b9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:25:52.950226Z","iopub.execute_input":"2025-09-25T18:25:52.950498Z","iopub.status.idle":"2025-09-25T18:53:55.084878Z","shell.execute_reply.started":"2025-09-25T18:25:52.950475Z","shell.execute_reply":"2025-09-25T18:53:55.084093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 16.4 Plot GAN training history\n\n# Load the history from the CSV file\nhistory_df_gan = pd.read_csv(\"history_log_gan.csv\")\n\nmock_history_gan = SimpleNamespace(history = history_df_gan.to_dict('list'))\n\n# Plot\nplot_gan_metrics(mock_history_gan, title_prefix = \"SRRGAN\")","metadata":{"_uuid":"9e75ac30-c108-49f7-b8c1-4fb229d4e3c2","_cell_guid":"d960bf58-cb27-4f4c-abbd-41d603999714","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:53:55.086726Z","iopub.execute_input":"2025-09-25T18:53:55.086948Z","iopub.status.idle":"2025-09-25T18:53:55.781813Z","shell.execute_reply.started":"2025-09-25T18:53:55.086931Z","shell.execute_reply":"2025-09-25T18:53:55.780959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 16.5 Save the final generator model\n\nSRRGAN.generator.save(\"model_gan.keras\")","metadata":{"_uuid":"11f5f45f-cb5b-49f1-a69d-2845e4508acc","_cell_guid":"4904e2a7-aac1-402f-b5a1-ee443c61b55e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:53:55.782612Z","iopub.execute_input":"2025-09-25T18:53:55.782827Z","iopub.status.idle":"2025-09-25T18:53:57.38074Z","shell.execute_reply.started":"2025-09-25T18:53:55.782809Z","shell.execute_reply":"2025-09-25T18:53:57.379919Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gan Evaluation\nOptimizing a **GAN** based on its loss can be unreliable due to the adversarial nature of the training.\nTo find the best model, I will evaluate the generator from each checkpoint (saved every 4 epochs) using the **LPIPS** metric. The best model is the one with the **lowest** LPIPS value.\n\nThe choice to use LPIPS instead of PSNR or SSIM metrics is due to the purpose of using the GAN: create images that are *perceptually realistic*, not just pixel-accurate. Since GANs are unstable, during the training, they can start producing \"bad\" artifacts. PSNR can miss GAN artifacts in localized zones. On the other hand, LPIPS compares images in a feature space. This correlates better with **human judgments** of perceptual similarity then PSNR or SSIM and it is more sensitive to unnatural patterns and textures than PSNR.","metadata":{"_uuid":"6c6ed578-0253-4f14-9522-9170d8a8ba61","_cell_guid":"f77dfe93-4ab3-456e-9895-886e321b6c6f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 16.6 Setting for Gan evaluation\n\n# Directory of saved checkpoints\nCHECKPOINT_DIR = \"gan_checkpoints\"\n\n# List of all saved checkpoint and sort them by epoch\ncheckpoint_paths = sorted(glob.glob(os.path.join(CHECKPOINT_DIR, \"*.keras\")))\n\nprint(f\"Found {len(checkpoint_paths)} checkpoints to evaluate.\")","metadata":{"_uuid":"21486b5a-9e3e-4797-8037-284d466c66a0","_cell_guid":"fcf593a7-f6d1-4395-a57d-01b832d70c6d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:53:57.382088Z","iopub.execute_input":"2025-09-25T18:53:57.382439Z","iopub.status.idle":"2025-09-25T18:53:57.388851Z","shell.execute_reply.started":"2025-09-25T18:53:57.382406Z","shell.execute_reply":"2025-09-25T18:53:57.387918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 16.7 Create LPIPS under float32 policy (Notebook's policy is float16, but the metric was built in float32)\n\norig_policy = tf.keras.mixed_precision.global_policy().name\ntf.keras.mixed_precision.set_global_policy(\"float32\")\nlpips_loss_fn = lpips_base_tf.LPIPS(base='vgg16', pre_norm=False)\nprint(\"LPIPS compute dtype:\", lpips_loss_fn.compute_dtype)\ntf.keras.mixed_precision.set_global_policy(orig_policy)  # restore mixed policy","metadata":{"_uuid":"9ed7edfd-fb29-468a-a3a4-6bd62340a4e0","_cell_guid":"73b5f6ce-657f-4f86-a80c-a9ee61dfd939","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:53:57.38985Z","iopub.execute_input":"2025-09-25T18:53:57.390248Z","iopub.status.idle":"2025-09-25T18:53:58.423194Z","shell.execute_reply.started":"2025-09-25T18:53:57.390221Z","shell.execute_reply":"2025-09-25T18:53:58.422461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 16.8 Evaluation loop \n\nresults = {}\n\nfor ckpt_path in checkpoint_paths:\n    print(f\"\\n--- Evaluating checkpoint: {os.path.basename(ckpt_path)} ---\")\n    model = load_model(ckpt_path, custom_objects=custom_objects)\n    # Generator is the one to evealuate\n    generator = model.generator\n\n    lpips_scores = []\n    for lr_batch, hr_batch in tqdm(test_ds, desc=\"Processing\", leave=False):\n        sr_batch = generator(lr_batch, training=False)\n\n        # Cast inputs to float32 so LPIPS receives float32 tensors (better to do it again...)\n        hr_batch = tf.cast(hr_batch, tf.float32)\n        sr_batch = tf.cast(sr_batch, tf.float32)\n\n        # Compute LPIPS\n        lpips_score_batch = lpips_loss_fn(hr_batch, sr_batch)\n        lpips_scores.extend(tf.reshape(lpips_score_batch, [-1]).numpy().tolist())\n\n    avg_lpips = float(np.mean(lpips_scores))\n    results[os.path.basename(ckpt_path)] = {\"LPIPS\": avg_lpips}","metadata":{"_uuid":"3207fcf3-95ef-4994-ba6d-7856e00f84e2","_cell_guid":"ab40c59d-a423-4ae0-8b27-9a2afa649b29","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:53:58.424045Z","iopub.execute_input":"2025-09-25T18:53:58.424732Z","iopub.status.idle":"2025-09-25T18:54:34.519311Z","shell.execute_reply.started":"2025-09-25T18:53:58.42471Z","shell.execute_reply":"2025-09-25T18:54:34.518626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 16.9 Gan final results\n\nsorted_results = sorted(results.items(), key=lambda item: item[1]['LPIPS'])\n\nbest_gan_filename = sorted_results[0][0]\nbest_gan_path = os.path.join(CHECKPOINT_DIR, best_gan_filename)\n\nprint(f\"\\nBest GAN model identified: {best_gan_filename}\")\nprint(f\"   LPIPS: {sorted_results[0][1]['LPIPS']:.4f} (Lower is better)\")\n\nprint(\"\\n--- Full Ranking (by LPIPS) ---\")\nprint(f\"{'Checkpoint':<25} | {'LPIPS':<10}\")\nprint(\"-\" * 40)\nfor name, scores in sorted_results:\n    print(f\"{name:<25} | {scores['LPIPS']:.4f}\")\nprint(\"-\" * 40)","metadata":{"_uuid":"8ef6d84b-36bb-47bf-9613-679913468d59","_cell_guid":"3ed9d110-9e43-4489-a2bd-29a4e93e325d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:54:34.520662Z","iopub.execute_input":"2025-09-25T18:54:34.520898Z","iopub.status.idle":"2025-09-25T18:54:34.526639Z","shell.execute_reply.started":"2025-09-25T18:54:34.520881Z","shell.execute_reply":"2025-09-25T18:54:34.526065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation & Visualization","metadata":{"_uuid":"4f6e1ead-6ed3-4a45-8b95-547827c1f18c","_cell_guid":"69267e2e-82ce-420c-b266-5e3902f7e8e0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 17 Load models + Evaluation\n\n# 17.1 load models\nmodel_mae = load_model(\"/kaggle/input/model-mae-and-maeperceptual/keras/default/1/model_mae.keras\",               custom_objects=custom_objects,   safe_mode=False) # Mae\nmodel_perceptual = load_model(\"/kaggle/input/model-mae-and-maeperceptual/keras/default/1/model_perceptual.keras\", custom_objects=custom_objects,   safe_mode=False) # Mae + Perceptual\n\nbest_gan_model_full = load_model(best_gan_path, custom_objects=custom_objects)  # GAN\nmodel_gan = best_gan_model_full.generator\n\n# 17.2 Evaluation of each model on the test data\nres_mae = model_mae.evaluate(test_ds, return_dict=True)\nres_perceptual = model_perceptual.evaluate(test_ds, return_dict=True)\n\n# 17.3 The GAN generator was not compiled with a loss... so re-compile it for evaluation\nmodel_gan.compile(loss=mae_only_loss, metrics=[psnr_metric, ssim_metric, pixel_loss_fn, perceptual_multi_layer])\nres_gan = model_gan.evaluate(test_ds, return_dict=True)\nprint(\"\\nModel evaluated.\")","metadata":{"_uuid":"dc970fd7-af95-459d-b13b-2c690180d371","_cell_guid":"ee1cbf17-2338-45f2-b27c-a8958167afcf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T18:54:34.527332Z","iopub.execute_input":"2025-09-25T18:54:34.528098Z","iopub.status.idle":"2025-09-25T18:55:44.061041Z","shell.execute_reply.started":"2025-09-25T18:54:34.528057Z","shell.execute_reply":"2025-09-25T18:55:44.060472Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation\n\nPSNR and SSIM don't always align with human perception of image quality. The LPIPS (Learned Perceptual Image Patch Similarity) metric is designed to be a better \"proxy\" for how humans perceive differences.\n\n*    **PSNR: A higher PSNR score is better** indicating that the generated image is more similar to the ground truth in terms of pixel-level accuracy.\n*    **SSIM: A higher SSIM score is better** indicating that the generated image is more similar to the ground truth in terms of perceived structural and visual quality.\n*    **LPIPS: A lower score is better**, indicating that the generated image is perceptually closer to the ground truth.","metadata":{"_uuid":"a56ba683-32a3-44ac-8d8d-8b240f36f958","_cell_guid":"0ab368ff-9ae7-4829-b16f-f29d0413443a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- Evaluation\n\n# 18.1 As before, ensure the policy is float32 for the LPIPS calculation.\norig_policy = tf.keras.mixed_precision.global_policy().name\ntf.keras.mixed_precision.set_global_policy(\"float32\")\nlpips_loss_fn = lpips_base_tf.LPIPS(base='vgg16', pre_norm=False)\ntf.keras.mixed_precision.set_global_policy(orig_policy)\n\ndef evaluate_lpips(model, dataset):\n    \"\"\"\n    Calculates the average LPIPS score for a given model on a dataset.\n    \"\"\"\n    all_lpips_scores = []\n    for lr_batch, hr_batch in tqdm(dataset, desc=f\"LPIPS for {model.name}\"):\n        sr_batch = model(lr_batch, training=False)\n        \n        # Cast to float32 for LPIPS metric\n        hr_batch_f32 = tf.cast(hr_batch, tf.float32)\n        sr_batch_f32 = tf.cast(sr_batch, tf.float32)\n\n        lpips_scores = lpips_loss_fn(hr_batch_f32, sr_batch_f32)\n        all_lpips_scores.extend(tf.reshape(lpips_scores, [-1]).numpy())\n        \n    return float(np.mean(all_lpips_scores))\n\n# 18.2 Calculate Mae for all models\nmae_val_mae_model = res_mae['loss'] \nmae_val_perceptual_model = res_perceptual['mean_absolute_error'] \nmae_val_gan_model = res_gan['mean_absolute_error']\n\n# 18.3 Calculate LPIPS for all models\nlpips_mae = evaluate_lpips(model_mae, test_ds)\nlpips_perceptual = evaluate_lpips(model_perceptual, test_ds)\nlpips_gan = evaluate_lpips(model_gan, test_ds)\n\n# 18.4 Display Final Comparison Table\nprint(\"\\n\" + \"=\"*70)\nprint(\" \" * 21 + \"FINAL MODEL COMPARISON\")\nprint(\"=\"*70)\nprint(f'{\"Metric\":<12} | {\"U-Net (MAE)\":<15} | {\"U-Net (Perceptual)\":<20} | {\"SRRGAN\":<15}')\nprint(\"-\"*70)\nprint(f'{\"PSNR\":<12} | {res_mae[\"psnr_metric\"]:.4f}{\" \":>8} | {res_perceptual[\"psnr_metric\"]:.4f}{\" \":>13} | {res_gan[\"psnr_metric\"]:.4f}')\nprint(f'{\"SSIM\":<12} | {res_mae[\"ssim_metric\"]:.4f}{\" \":>9} | {res_perceptual[\"ssim_metric\"]:.4f}{\" \":>14} | {res_gan[\"ssim_metric\"]:.4f}')\nprint(f'{\"MAE (Pixel)\":<12} | {mae_val_mae_model:.4f}{\" \":>9} | {mae_val_perceptual_model:.4f}{\" \":>14} | {mae_val_gan_model:.4f}')\nprint(f'{\"LPIPS\":<12} | {lpips_mae:.4f}{\" \":>9} | {lpips_perceptual:.4f}{\" \":>14} | {lpips_gan:.4f}')\nprint(\"=\"*70)","metadata":{"_uuid":"0a5235cc-5310-4341-893c-1078de18d141","_cell_guid":"abc2ef9b-3773-48f6-93a0-bae53dc61204","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T19:38:28.609617Z","iopub.execute_input":"2025-09-25T19:38:28.609894Z","iopub.status.idle":"2025-09-25T19:38:41.385698Z","shell.execute_reply.started":"2025-09-25T19:38:28.609874Z","shell.execute_reply":"2025-09-25T19:38:41.384911Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Patch Visualization","metadata":{"_uuid":"326e17f4-ac6c-4188-be9d-429c9bb0e27f","_cell_guid":"7afec47c-8895-406d-bfd6-af8415a18591","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 19 Preparation for the Patch Visualization\n\n# 19.1 Denormalization\ndef denorm(img):\n    img_f32 = tf.cast(img, tf.float32)\n    return tf.clip_by_value((img_f32 + 1.0) / 2.0, 0, 1)\n\n# 19.2 Select random images from test data\nnum_images_to_show = 10\nsample_paths = random.sample(test_paths, num_images_to_show)\nplt.figure(figsize=(50, 100)) # Adjusted figsize for better aspect ratio\n\n# 19.3.1 Loop through each of the selected images\nfor i, sample_path_hr in enumerate(sample_paths):\n    print(f\"--- Processing patch from image {i+1}/{num_images_to_show} ---\")\n\n    # 19.3.2 Preparation steps are now inside the loop\n    hr_full_image = tf.image.convert_image_dtype(                                  # Get image\n        tf.io.decode_png(tf.io.read_file(sample_path_hr), channels=CH),\n        tf.float32\n    )\n    hr_patch_gt = tf.image.random_crop(hr_full_image, size=[PATCH_H, PATCH_W, CH])                    # Use random crop for more variety\n    lr_patch_degraded_0_1 = degradation_pipeline_tf(hr_patch_gt, is_training=False).numpy()           # Apply degradation\n    lr_patch_input = (lr_patch_degraded_0_1 * 2.0) - 1.0 \n    \n    # 19.3.3 Model predictions\n    sr_patch_predicted_mae = model_mae.predict(lr_patch_input[None, ...], verbose=0)[0]\n    sr_patch_predicted_perceptual = model_perceptual.predict(lr_patch_input[None, ...], verbose=0)[0]\n    sr_patch_predicted_gan = model_gan.predict(lr_patch_input[None, ...], verbose=0)[0]\n\n    # 19.3.4 Plotting row of patch images \n    \n    # 19.3.4.1 LR Input\n    ax = plt.subplot(num_images_to_show, 5, i * 5 + 1)\n    ax.imshow(denorm(lr_patch_input))\n    ax.axis(\"off\")\n    if i == 0: ax.set_title(f\"LR Input\\n{PATCH_LR_H}x{PATCH_LR_W}\", fontsize = 40)  \n\n    # 19.3.4.2 U-Net with MAE\n    ax = plt.subplot(num_images_to_show, 5, i * 5 + 2)\n    ax.imshow(denorm(sr_patch_predicted_mae))\n    ax.axis(\"off\")\n    if i == 0: ax.set_title(f\"U-Net MAE\\n{PATCH_H}x{PATCH_W}\", fontsize = 40)\n\n    # 19.3.4.3 U-Net with Perceptual Loss\n    ax = plt.subplot(num_images_to_show, 5, i * 5 + 3)\n    ax.imshow(denorm(sr_patch_predicted_perceptual))\n    ax.axis(\"off\")\n    if i == 0: ax.set_title(f\"U-Net Perceptual\\n{PATCH_H}x{PATCH_W}\", fontsize = 40)\n\n    # 19.3.4.4 SRRGAN\n    ax = plt.subplot(num_images_to_show, 5, i * 5 + 4)\n    ax.imshow(denorm(sr_patch_predicted_gan))\n    ax.axis(\"off\")\n    if i == 0: ax.set_title(f\"SRRGAN\\n{PATCH_H}x{PATCH_W}\", fontsize = 40)\n\n    # 19.3.4.5 Original HR Patch (Ground Truth)\n    ax = plt.subplot(num_images_to_show, 5, i * 5 + 5)\n    ax.imshow(hr_patch_gt)\n    ax.axis(\"off\")\n    if i == 0: ax.set_title(f\"Original HR (GT)\\n{PATCH_H}x{PATCH_W}\", fontsize = 40)\n    \n# 19.4 Show the final plot\nplt.subplots_adjust(wspace=0.05, hspace=0.05)\n\nplt.savefig(\"model_comparison_patch.png\", dpi=400, bbox_inches='tight') \nplt.show()","metadata":{"_uuid":"61143f01-9f6c-4fde-9b6f-daa9962ac570","_cell_guid":"223c9577-b705-4968-9f44-fa2ba27e6e02","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T19:39:10.11749Z","iopub.execute_input":"2025-09-25T19:39:10.117758Z","iopub.status.idle":"2025-09-25T19:40:48.14594Z","shell.execute_reply.started":"2025-09-25T19:39:10.117739Z","shell.execute_reply":"2025-09-25T19:40:48.144566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Full image inference\n\nThe following function handles inference on full-sized images, which can be tricky. Here is the general \"story\":\n* **Padding (20.1-20.3)**:An image's dimensions might not be perfectly divisible by the patch stride, which would cause issues when tiling the image. To solve this, padding is added to **ensure** the dimensions are a perfect multiple of the stride. This is done using `mode=\"reflect\"`, which reflects pixels from the edge to create a seamless border.\n* **Extract patches (20.4)**: `tf.image.extract_patches` performs a sliding window operation, returning a single batch of all the 128x128 LR patches.\n\n* **Predict (20.5)**: The model **predicts** on the entire batch of patches at once.\n* **Reconstruction & Blending (20.6-20.10)**: Simply placing predicted HR patches side-by-side can create visible seams because the edges might not perfectly match. The solution is to **blend** the patches in their overlapping **areas**.\n\n    * A **\"Hann Window\" (20.7)** is used for blending. It has a value of 1.0 in the center and smoothly fades to 0.0 at the edges.\n\n    * Each predicted HR patch is **multiplied** by the Hann window **(20.8)** before being added to the final output canvas (`final_hr`). This way, the center of each patch contributes fully, while the edges contribute less.\n\n    * In overlapping regions, a pixel receives contributions from multiple patches. A `weight_map` **(20.9)** keeps track of the sum of these Hann window values at every pixel. Dividing by this map performs a weighted average of the predictions, creating a smooth transition.\n\n    * Finally, the initial padding is **cropped** off **(20.10)**.","metadata":{"_uuid":"3d57d76f-2767-4140-aae5-15fe54e8e09f","_cell_guid":"0bc2ba07-c691-4398-8764-ff9efd6f607f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 20 Full Image Inference Functions\n\ndef predict_full_image_tiled(model, lr_image_norm, patch_size=128, overlap=32, batch_size=8):\n    \"\"\"\n    model: the trained super-resolution model.\n    lr_image_norm: the low-resolution input image normalized to [-1, 1].\n    patch_size: the size of LR patches to predict on.\n    overlap: the overlap between patches.\n    batch_size: batch size for model prediction.\n    \"\"\"\n    # 20.1 Get image dimensions and calculate stride\n    lr_h, lr_w, C = lr_image_norm.shape\n    stride = patch_size - overlap\n\n    # 20.2 Calc padding needed for the image to be divisible by the stride\n    # This ensures that the entire image is covered by patches.\n    n_patches_h = math.ceil(max(0, lr_h - patch_size) / stride) + 1\n    n_patches_w = math.ceil(max(0, lr_w - patch_size) / stride) + 1\n    pad_h = (n_patches_h - 1) * stride + patch_size - lr_h\n    pad_w = (n_patches_w - 1) * stride + patch_size - lr_w\n    \n    # 20.3 Apply reflection padding to minimize edge artifacts\n    lr_padded = np.pad(lr_image_norm, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n\n    # 20.4 Extract patches\n    # The input image is expanded to a 4D tensor (batch, height, width, channels)\n    lr_padded_tensor = tf.convert_to_tensor(lr_padded[None, ...], dtype=tf.float32)  # shape (1, H, W, C)\n    patches = tf.image.extract_patches(\n        images=lr_padded[None, ...],\n        sizes=[1, patch_size, patch_size, 1],\n        strides=[1, stride, stride, 1],\n        rates=[1, 1, 1, 1],\n        padding='VALID'\n    )\n    # Reshape the extracted patches into a batch that the model can process\n    patches = tf.reshape(patches, [-1, patch_size, patch_size, C])\n    patches = tf.cast(patches, tf.float32)\n\n    # 20.5 Run model prediction on the batch of patches (already in  [-1, 1] range)\n    preds_norm = model.predict(patches, batch_size=batch_size, verbose=0)   # Output is [-1, 1]\n\n    # 20.6 Reconstruct the full HR image from the predicted HR patches\n    patch_hr_size = patch_size * UPSCALE_FACTOR\n    stride_hr = stride * UPSCALE_FACTOR\n    hr_h, hr_w = lr_padded.shape[0] * UPSCALE_FACTOR, lr_padded.shape[1] * UPSCALE_FACTOR\n    \n    # Create an empty canvas for the final image and a weight map for blending\n    final_hr = np.zeros((hr_h, hr_w, C), dtype=np.float32)\n    weight_map = np.zeros_like(final_hr)\n\n    # 20.7 Create a 2D Hann window for smooth blending\n    # The window gives more weight to the pixels in the center of a patch and less to the edges.\n    hann_1d = np.hanning(patch_hr_size)\n    window_2d = np.outer(hann_1d, hann_1d)[..., None]\n\n    # 20.8 Loop through patches, apply the window, and add them to the canvas\n    idx = 0\n    for iy in range(n_patches_h):\n        for ix in range(n_patches_w):\n            y, x = iy * stride_hr, ix * stride_hr\n            # Add the predicted patch, weighted by the Hann window\n            final_hr[y:y+patch_hr_size, x:x+patch_hr_size, :] += preds_norm[idx] * window_2d\n            # Add the window itself to the weight map\n            weight_map[y:y+patch_hr_size, x:x+patch_hr_size, :] += window_2d\n            idx += 1\n\n    # 20.9 Normalize the reconstructed image by the weight map\n    # This averages the overlapping regions weighted by the Hann window.\n    final_hr = final_hr / (weight_map + 1e-8)\n    \n    # 20.10 Crop the image back to its original size, removing the padding\n    orig_hr_h, orig_hr_w = lr_h * UPSCALE_FACTOR, lr_w * UPSCALE_FACTOR\n    final_hr_cropped = final_hr[:orig_hr_h, :orig_hr_w, :]\n\n    return final_hr_cropped # The output is still in [-1, 1] range","metadata":{"_uuid":"209e5be9-84d9-4634-9a32-fa1712d3dd2d","_cell_guid":"5888aa5d-2d23-4f76-bb95-f9a626a7a583","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T19:40:48.148315Z","iopub.execute_input":"2025-09-25T19:40:48.148722Z","iopub.status.idle":"2025-09-25T19:40:48.161153Z","shell.execute_reply.started":"2025-09-25T19:40:48.148699Z","shell.execute_reply":"2025-09-25T19:40:48.160405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Full Image Visualization","metadata":{"_uuid":"252277bd-d15d-4854-b7d6-03047636c270","_cell_guid":"20dac95f-ff39-4046-920d-fe0eb3266ed2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 21 Final Full Image Visualization\n\n# 21.1 Loaded model group\nmodels_to_visualize = {\n    \"U-Net MAE\": model_mae,\n    \"U-Net Perceptual\": model_perceptual,\n    \"SRRGAN\": model_gan\n}\n\n# 21.2 Select random images\nnum_images_to_show = 10 \nsample_paths = random.sample(test_paths, num_images_to_show)\n\n# 21.3 Create a figure and a grid of subplots\nfig, axes = plt.subplots(\n    nrows=num_images_to_show,\n    ncols=5,\n    figsize=(25, 5 * num_images_to_show) \n)\n\nif num_images_to_show == 1:\n    axes = np.array([axes])\n\n# 21.4 Loop for prediction and plotting \nfor i, sample_path_full in enumerate(sample_paths):\n    print(f\"\\n--- Processing image {i+1}/{num_images_to_show}: {sample_path_full.split('/')[-1]} ---\")\n\n    # 21.5 Load and prepare the HR image\n    hr_full_original = tf.image.convert_image_dtype(tf.io.decode_png(tf.io.read_file(sample_path_full), channels=CH), tf.float32).numpy()\n    hr_full_tensor = tf.convert_to_tensor(hr_full_original, dtype=tf.float32)\n\n    # Create the LR input, preserving the aspect ratio\n    lr_full_input_degraded = degrade_full_image(hr_full_tensor).numpy()\n\n    lr_model_input = (lr_full_input_degraded * 2.0) - 1.0\n\n    # 21.6 Generate super-resolved images\n    sr_images = {}\n    for name, model in models_to_visualize.items():\n        sr_image_norm = predict_full_image_tiled(model, lr_model_input, patch_size=PATCH_LR_W, overlap=32, batch_size=BATCH_SIZE)\n        sr_images[name] = np.clip((sr_image_norm + 1.0) / 2.0, 0.0, 1.0).astype('float32')\n    \n    # 21.7 Plotting the images\n    axes[i, 0].imshow(lr_full_input_degraded.astype('float32'))\n    \n    for j, (name, sr_img) in enumerate(sr_images.items()):\n        axes[i, j + 1].imshow(sr_img)\n\n    axes[i, 4].imshow(hr_full_original.astype('float32'))\n\n# 21.8 Set titles and turn off axes\nfor i in range(num_images_to_show):\n    for j in range(5):\n        axes[i, j].axis(\"off\")\n        if i == 0:\n            if j == 0:\n                axes[i, j].set_title(f\"Degraded LR Input\", fontsize=20)\n            elif j == 4:\n                axes[i, j].set_title(f\"Original High-Res (GT)\", fontsize=20)\n            else:\n                model_name = list(models_to_visualize.keys())[j-1]\n                axes[i, j].set_title(f\"{model_name}\", fontsize=20)\n\nplt.tight_layout(pad=0.1, w_pad=0.5, h_pad=0.5)\nplt.savefig(\"model_comparison_FULL_final.png\", dpi=300, bbox_inches='tight') \nplt.show()","metadata":{"_uuid":"75c46754-e62a-4106-b315-4d0f027e3144","_cell_guid":"8eca009a-9c18-4204-8e60-4498281489ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-25T19:40:48.162125Z","iopub.execute_input":"2025-09-25T19:40:48.162495Z","iopub.status.idle":"2025-09-25T19:41:36.155287Z","shell.execute_reply.started":"2025-09-25T19:40:48.162458Z","shell.execute_reply":"2025-09-25T19:41:36.154507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparision with Bilinear upsample","metadata":{}},{"cell_type":"code","source":"# ---  Qualitative Comparison\n# We will visualize the comparison of different images comparing different models: Bilinear upsample and Gan\n\nnum_images_to_show = 10 \nmodels_to_visualize = {\"SRRGAN\": model_gan} \n\n# Figure\nfig, axes = plt.subplots(\n    nrows=num_images_to_show,\n    ncols=4,\n    figsize=(20, 5 * num_images_to_show)\n)\n\n# Ensure axes is a 2D array even if num_images_to_show is 1, for consistent indexing\nif num_images_to_show == 1:\n    axes = np.array([axes])\n\n# Select random sample\nsample_paths = random.sample(test_paths, num_images_to_show)\n\nfor i, sample_path in enumerate(sample_paths):\n    print(f\"--- Generating comparison for image {i+1}/{num_images_to_show}: {os.path.basename(sample_path)} ---\")\n\n    # 1 Original HR Image (float32 - [0, 1])\n    hr_original_tensor = tf.image.convert_image_dtype(\n        tf.io.decode_png(tf.io.read_file(sample_path), channels=CH),\n        tf.float32\n    )\n    hr_original_np = hr_original_tensor.numpy()\n    hr_shape = tf.shape(hr_original_tensor)[:2] \n\n    # 2 Low-Resolution Degraded Image ([0, 1] range)\n    lr_degraded_tensor = degrade_full_image(hr_original_tensor)\n    lr_degraded_np = lr_degraded_tensor.numpy()\n\n    # 3 Create a simple Bilinear Upsampled Version\n    # Resize the LR image back up to the original HR dimensions using bilinear interpolation\n    hr_bilinear_tensor = tf.image.resize(\n        lr_degraded_tensor,\n        hr_shape,\n        method=tf.image.ResizeMethod.BILINEAR\n    )\n    # Clip values for valid [0, 1] range for plotting\n    hr_bilinear_np = tf.clip_by_value(hr_bilinear_tensor.numpy(), 0, 1)\n\n    # 4 GAN Inference\n    # 4.1 Normalize the LR image from [0, 1] to [-1, 1] for the model input\n    lr_model_input = (lr_degraded_np * 2.0) - 1.0\n    # 4.2 Predict full images\n    sr_gan_norm_np = predict_full_image_tiled(\n        model_gan,\n        lr_model_input,\n        patch_size=PATCH_LR_W,    \n        overlap=32,               \n        batch_size=BATCH_SIZE     \n    )\n    # 4.3 Denormalize the GAN output from [-1, 1] back to [0, 1] for plotting\n    sr_gan_np = np.clip((sr_gan_norm_np + 1.0) / 2.0, 0, 1)\n\n    \n    # 5 Plot results \n    \n    # Low-Resolution Degraded Input\n    axes[i, 0].imshow(lr_degraded_np)\n    axes[i, 0].axis('off')\n\n    # Simple Bilinear Upsample\n    axes[i, 1].imshow(hr_bilinear_np)\n    axes[i, 1].axis('off')\n\n    # GAN Inference Output\n    axes[i, 2].imshow(sr_gan_np)\n    axes[i, 2].axis('off')\n\n    # Original High-Resolution (Ground Truth)\n    axes[i, 3].imshow(hr_original_np)\n    axes[i, 3].axis('off')\n\n    # Titles for the first row of images\n    if i == 0:\n        axes[i, 0].set_title(f\"Low-Res Degraded\", fontsize=16)\n        axes[i, 1].set_title(f\"Upsampled (Bilinear)\", fontsize=16)\n        axes[i, 2].set_title(f\"SRRGAN Inference\", fontsize=16)\n        axes[i, 3].set_title(f\"Original HR (GT)\", fontsize=16)\n\n# Layout \nplt.tight_layout(pad=0.5)\n\n# Save\nplt.savefig(\"final_model_comparison.png\", dpi=300, bbox_inches='tight')\n\n# Display \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}